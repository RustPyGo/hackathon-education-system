# ğŸš€ AI Education Question Generator - SYNC OPTIMIZED

Há»‡ thá»‘ng AI sinh cÃ¢u há»i tráº¯c nghiá»‡m tá»« file PDF **tá»‘i Æ°u cho xá»­ lÃ½ Ä‘á»“ng bá»™** vá»›i kháº£ nÄƒng táº¡o **300 cÃ¢u há»i/láº§n gá»i API**.

## âš¡ Tá»‘i Æ¯u HÃ³a ChÃ­nh

### ğŸ¯ Hiá»‡u NÄƒng
- **300 cÃ¢u há»i/láº§n** (tÄƒng tá»« 200)
- **15 files/láº§n** (tÄƒng tá»« 10)
- **Xá»­ lÃ½ song song** vá»›i ThreadPoolExecutor
- **Batch processing tá»‘i Æ°u** (5-15 cÃ¢u/batch)
- **Model nhanh hÆ¡n** (gpt-3.5-turbo)
- **Timeout ngáº¯n** (30s/request)

### ğŸ’¾ Cache ThÃ´ng Minh
- **Memory cache** + **Disk cache**
- **Thread-safe** vá»›i locking
- Cache theo **file_name**
- **Auto cleanup** vÃ  **cache rotation**

### ğŸ”§ Kiáº¿n TrÃºc
- **Chá»‰ sync** (loáº¡i bá» async overhead)
- **Single worker** (trÃ¡nh cache conflicts)
- **Optimized connection pooling**
- **Fast JSON parsing**

## ğŸ“ Files Má»›i

```
ai/
â”œâ”€â”€ server_sync_optimized.py      # Server tá»‘i Æ°u (MAIN)
â”œâ”€â”€ genQ.py                       # ÄÃ£ thÃªm batch processing tá»‘i Æ°u
â”œâ”€â”€ test_sync_optimized.py        # Test script cho 300 cÃ¢u
â”œâ”€â”€ run_sync_optimized.py         # Launch script
â”œâ”€â”€ requirements_optimized.txt    # Dependencies tá»‘i Æ°u
â””â”€â”€ README_SYNC_OPTIMIZED.md      # File nÃ y
```

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### 1. Khá»Ÿi Äá»™ng Nhanh
```bash
cd ai
python run_sync_optimized.py
```

### 2. Khá»Ÿi Äá»™ng Thá»§ CÃ´ng
```bash
# CÃ i dependencies
pip install -r requirements_optimized.txt

# Cháº¡y server
python server_sync_optimized.py
```

### 3. Test Hiá»‡u NÄƒng
```bash
# Update TEST_FILES trong test_sync_optimized.py vá»›i URLs tháº­t
python test_sync_optimized.py
```

## ğŸ“¡ API Endpoints

### ğŸ¯ Main Endpoint (OPTIMIZED)
```http
POST /api/generate-questions-sync
Content-Type: application/json

{
  "files": [
    {
      "url": "https://example.com/doc1.pdf",
      "file_name": "document1.pdf"
    },
    {
      "url": "https://example.com/doc2.pdf", 
      "file_name": "document2.pdf"
    }
  ],
  "project_id": "project_123",
  "total_questions": 300,
  "name": "Test 300 Questions"
}
```

### ğŸ“Š Response Format
```json
{
  "overview": "ÄÃ£ táº¡o thÃ nh cÃ´ng 300/300 cÃ¢u há»i...",
  "quiz": [
    {
      "question": "CÃ¢u há»i 1...",
      "type": "multiple_choice",
      "hint": "Gá»£i Ã½...",
      "correct_answer": "A",
      "options": [
        {"answer": "A", "reason": "ÄÃºng vÃ¬..."},
        {"answer": "B", "reason": "Sai vÃ¬..."},
        {"answer": "C", "reason": "Sai vÃ¬..."},
        {"answer": "D", "reason": "Sai vÃ¬..."}
      ]
    }
  ],
  "metadata": {
    "project_id": "project_123",
    "name": "Test 300 Questions",
    "total_questions": 300,
    "files_processed": ["document1.pdf", "document2.pdf"],
    "cached_files": ["document1.pdf"],
    "new_files": ["document2.pdf"],
    "failed_files": [],
    "questions_per_file": {
      "document1.pdf": 150,
      "document2.pdf": 150
    },
    "processing_time": "45.2s",
    "cache_usage": "50.0% cache hit rate"
  }
}
```

### ğŸ”§ Utility Endpoints
```http
GET  /api/health           # Health check
GET  /api/cache/info       # Cache thÃ´ng tin
DELETE /api/cache/clear    # XÃ³a cache
```

## âš¡ Tá»‘i Æ¯u HÃ³a Chi Tiáº¿t

### 1. Batch Processing
```python
# Tá»‘i Æ°u batch size theo sá»‘ cÃ¢u há»i
if num_questions <= 10:
    batch_size = num_questions  # Táº¡o má»™t láº§n
elif num_questions <= 50:
    batch_size = 10  # Batch 10 cÃ¢u
else:
    batch_size = 15  # Batch 15 cÃ¢u cho sá»‘ lÆ°á»£ng lá»›n
```

### 2. Parallel File Processing
```python
# ThreadPoolExecutor vá»›i sá»‘ threads tá»‘i Æ°u
max_workers = min(len(files), os.cpu_count() or 4, 8)
```

### 3. Optimized Caching
```python
# Memory cache + Disk cache vá»›i thread safety
class OptimizedMultiFileCache:
    def __init__(self):
        self.memory_cache = {}
        self.cache_lock = threading.Lock()
```

### 4. Fast API Calls
```python
# Timeout ngáº¯n + gpt-3.5-turbo
data = {
    'model': 'gpt-3.5-turbo',  # Nhanh hÆ¡n gpt-4
    'timeout': 30  # 30s timeout
}
```

## ğŸ“ˆ Hiá»‡u NÄƒng Má»¥c TiÃªu

| Metric | Target | Optimized |
|--------|--------|-----------|
| Max questions | 200 | **300** |
| Max files | 10 | **15** |
| Processing mode | Sync + Async | **Sync only** |
| Batch size | 5-8 | **5-15** |
| Cache | File-based | **Memory + Disk** |
| Parallel processing | No | **Yes** |
| Target time (300Q) | N/A | **â‰¤5 minutes** |

## ğŸ§ª Testing

### Performance Test Suite
```bash
python test_sync_optimized.py
```

### Test Cases
- âœ… Small batch (50 questions)
- âœ… Medium batch (150 questions) 
- âœ… **Large batch (300 questions)**
- âœ… Cache performance
- âœ… Error handling
- âœ… Parallel processing

### Expected Results
```
ğŸ“Š PERFORMANCE SUMMARY
âœ… SMALL: 50Q in 12.3s (4.1 Q/s)
âœ… MEDIUM: 150Q in 32.1s (4.7 Q/s)
âœ… LARGE: 300Q in 67.8s (4.4 Q/s)

ğŸ‰ PERFORMANCE TARGET MET!
Target: â‰¤300s for 300 questions
Actual: 67.8s
Status: PASS âœ…
```

## ğŸ”§ Configuration

### Environment Variables (.env)
```bash
OPENAI_API_KEY=your_openai_api_key_here
```

### Server Configuration
```python
# server_sync_optimized.py
app = FastAPI(
    title="AI Education Question Generator API - SYNC OPTIMIZED",
    version="2.0.0"
)

# Chá»‰ 1 worker Ä‘á»ƒ trÃ¡nh cache conflicts
uvicorn.run(app, host="0.0.0.0", port=8000, workers=1)
```

## ğŸš¨ Migration tá»« Version CÅ©

### 1. Thay Ä‘á»•i endpoint
```python
# CÅ©
POST /api/generate-questions-sync  # Max 200 cÃ¢u

# Má»›i  
POST /api/generate-questions-sync  # Max 300 cÃ¢u (same endpoint, more capacity)
```

### 2. Thay Ä‘á»•i request
```python
# CÅ©
"total_questions": 200  # Max

# Má»›i
"total_questions": 300  # Max
```

### 3. Táº¯t async (náº¿u Ä‘ang dÃ¹ng)
```python
# Endpoint async khÃ´ng cÃ²n trong version tá»‘i Æ°u
# Chá»‰ sá»­ dá»¥ng sync endpoint
```

## ğŸ› Troubleshooting

### Common Issues

1. **"Max 300 questions exceeded"**
   ```python
   # Giáº£m sá»‘ cÃ¢u há»i xuá»‘ng â‰¤ 300
   "total_questions": 300
   ```

2. **"Timeout after 30s"**
   ```python
   # File quÃ¡ lá»›n hoáº·c internet cháº­m
   # Thá»­ vá»›i file nhá» hÆ¡n hoáº·c Ã­t files hÆ¡n
   ```

3. **"Cache conflicts"**
   ```bash
   # Clear cache vÃ  restart
   curl -X DELETE http://localhost:8000/api/cache/clear
   ```

4. **Memory issues**
   ```python
   # Giáº£m sá»‘ files Ä‘á»“ng thá»i
   "files": [...] # Max 15 files
   ```

## ğŸ“Š Monitoring

### Health Check
```bash
curl http://localhost:8000/api/health
```

### Cache Status  
```bash
curl http://localhost:8000/api/cache/info
```

### Performance Metrics
- Questions per second (Q/s)
- Cache hit rate (%)
- Processing time per batch
- Memory usage
- Thread utilization

## ğŸ¯ Best Practices

### 1. File Management
- Sá»­ dá»¥ng file_name unique Ä‘á»ƒ cache hiá»‡u quáº£
- Limit file size < 50MB per file
- PDF quality tá»‘t cho text extraction

### 2. Question Distribution
- Chia Ä‘á»u cÃ¢u há»i giá»¯a cÃ¡c files
- Min 5 cÃ¢u/file, max khÃ´ng giá»›i háº¡n
- Balance content quality vs quantity

### 3. Caching Strategy
- Äá»ƒ cache warm-up vá»›i files thÆ°á»ng dÃ¹ng
- Clear cache Ä‘á»‹nh ká»³ Ä‘á»ƒ trÃ¡nh stale data
- Monitor cache size Ä‘á»ƒ trÃ¡nh disk full

### 4. Error Handling
- Xá»­ lÃ½ tá»«ng file Ä‘á»™c láº­p
- Continue processing khi 1 file fail
- Return partial results vá»›i error details

## ğŸš€ Roadmap

### Version 2.1 (Future)
- [ ] 500 cÃ¢u há»i/láº§n
- [ ] 20 files/láº§n  
- [ ] GPU acceleration
- [ ] Real-time processing status
- [ ] Advanced caching strategies

### Version 2.2 (Future)
- [ ] Multiple question types
- [ ] Custom difficulty levels
- [ ] Bulk export formats
- [ ] Analytics dashboard

---

**ğŸ‰ Ready to generate 300 questions at lightning speed!**

LiÃªn há»‡: AI Assistant Team
