import os
import json
import requests
from pathlib import Path
from dotenv import load_dotenv
import hashlib
from datetime import datetime
import subprocess
import sys

# Import cho RAG
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle
except ImportError:
    print("‚ùå Thi·∫øu m·ªôt s·ªë th∆∞ vi·ªán c·∫ßn thi·∫øt. ƒêang c√†i ƒë·∫∑t...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers", "scikit-learn", "numpy"])
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle

# Load environment variables
load_dotenv()

class PDFQuestionGenerator:
    def __init__(self):
        """Kh·ªüi t·∫°o h·ªá th·ªëng t·∫°o c√¢u h·ªèi t·ª´ PDF"""
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y OPENAI_API_KEY trong file .env")
            sys.exit(1)
        
        # Kh·ªüi t·∫°o model embedding
        print("üîÑ ƒêang t·∫£i m√¥ h√¨nh embedding...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Bi·∫øn l∆∞u tr·ªØ
        self.pdf_content = ""
        self.chunks = []
        self.embeddings = []
        self.document_summary = ""
        
        print("‚úÖ H·ªá th·ªëng t·∫°o c√¢u h·ªèi ƒë√£ s·∫µn s√†ng!")
    
    def convert_pdf_to_text(self, pdf_path):
        """Chuy·ªÉn ƒë·ªïi PDF sang text s·ª≠ d·ª•ng pdfToText.py"""
        try:
            print(f"üìÑ ƒêang chuy·ªÉn ƒë·ªïi PDF: {pdf_path}")
            
            # Import pdfToText module
            import importlib.util
            spec = importlib.util.spec_from_file_location("pdfToText", "pdfToText.py")
            pdf_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(pdf_module)
            
            # T·∫°o file output t·∫°m th·ªùi
            temp_output = f"temp_gen_pdf_{hashlib.md5(pdf_path.encode()).hexdigest()[:8]}.txt"
            
            # Chuy·ªÉn ƒë·ªïi PDF
            success = pdf_module.pdf_to_text(pdf_path, temp_output)
            
            if success and os.path.exists(temp_output):
                with open(temp_output, 'r', encoding='utf-8') as f:
                    self.pdf_content = f.read()
                
                # X√≥a file t·∫°m
                os.remove(temp_output)
                
                # Ki·ªÉm tra n·ªôi dung
                if len(self.pdf_content.strip()) == 0:
                    print("‚ùå PDF kh√¥ng ch·ª©a text c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c")
                    return False
                
                # Th·ªëng k√™
                error_pages = self.pdf_content.count("[L·ªñI TR√çCH XU·∫§T:")
                empty_pages = self.pdf_content.count("[TRANG TR·ªêNG")
                
                print(f"‚úÖ Chuy·ªÉn ƒë·ªïi PDF th√†nh c√¥ng!")
                print(f"üìä Th·ªëng k√™ n·ªôi dung:")
                print(f"   - ƒê·ªô d√†i: {len(self.pdf_content):,} k√Ω t·ª±")
                if error_pages > 0:
                    print(f"   - ‚ö†Ô∏è Trang l·ªói: {error_pages}")
                if empty_pages > 0:
                    print(f"   - üìù Trang tr·ªëng: {empty_pages}")
                
                return True
            else:
                print("‚ùå L·ªói chuy·ªÉn ƒë·ªïi PDF")
                return False
                
        except Exception as e:
            print(f"‚ùå L·ªói: {str(e)}")
            return False
    
    def clean_text(self, text):
        """L√†m s·∫°ch text v√† lo·∫°i b·ªè ph·∫ßn kh√¥ng c·∫ßn thi·∫øt"""
        import re
        
        # Lo·∫°i b·ªè header trang
        text = re.sub(r'={50}\nTRANG \d+\n={50}', '', text)
        
        # Lo·∫°i b·ªè th√¥ng b√°o l·ªói
        text = re.sub(r'\[L·ªñI TR√çCH XU·∫§T:.*?\]', '', text)
        text = re.sub(r'\[TRANG TR·ªêNG.*?\]', '', text)
        
        # Lo·∫°i b·ªè d√≤ng tr·ªëng li√™n ti·∫øp
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def create_chunks(self, text, chunk_size=1000, overlap=100):
        """Chia text th√†nh chunks l·ªõn h∆°n ƒë·ªÉ ph√¢n t√≠ch to√†n di·ªán"""
        cleaned_text = self.clean_text(text)
        words = cleaned_text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if len(chunk.strip()) > 100:  # Chunk t·ªëi thi·ªÉu 100 k√Ω t·ª±
                chunks.append(chunk.strip())
        
        print(f"üìö ƒê√£ t·∫°o {len(chunks)} chunks ƒë·ªÉ ph√¢n t√≠ch")
        return chunks
    
    def create_embeddings(self):
        """T·∫°o embeddings cho c√°c chunks"""
        print("üîÑ ƒêang t·∫°o chunks v√† embeddings...")
        
        # T·∫°o chunks
        self.chunks = self.create_chunks(self.pdf_content)
        
        if not self.chunks:
            print("‚ùå Kh√¥ng c√≥ chunks h·ª£p l·ªá")
            return False
        
        # T·∫°o embeddings
        self.embeddings = self.embedding_model.encode(self.chunks)
        
        print(f"‚úÖ ƒê√£ t·∫°o embeddings cho {len(self.chunks)} chunks")
        return True
    
    def call_openai_api(self, prompt, max_tokens=2000):
        """G·ªçi OpenAI API v·ªõi error handling t·ªët h∆°n"""
        try:
            # Ki·ªÉm tra ƒë·ªô d√†i prompt
            if len(prompt) > 12000:  # Gi·ªõi h·∫°n an to√†n
                print(f"‚ö†Ô∏è Prompt qu√° d√†i ({len(prompt)} chars), c·∫Øt ng·∫Øn...")
                prompt = prompt[:12000] + "..."
            
            url = "https://api.openai.com/v1/chat/completions"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.openai_api_key}"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": min(max_tokens, 2000),  # Gi·ªõi h·∫°n max_tokens
                "temperature": 0.7
            }
            
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            # Ki·ªÉm tra status code
            if response.status_code != 200:
                print(f"‚ùå OpenAI API error: {response.status_code}")
                print(f"Response: {response.text}")
                return None
                
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                print(f"‚ùå L·ªói API response: {result}")
                return None
                
        except Exception as e:
            print(f"‚ùå L·ªói g·ªçi API: {str(e)}")
            return None
    
    def generate_document_summary(self):
        """T·∫°o t√≥m t·∫Øt t·ªïng quan t√†i li·ªáu"""
        print("üìù ƒêang t·∫°o t√≥m t·∫Øt t√†i li·ªáu...")
        
        # L·∫•y n·ªôi dung quan tr·ªçng (gi·ªõi h·∫°n ƒë·ªô d√†i)
        important_content = ""
        if len(self.chunks) >= 2:
            important_content = self.chunks[0][:1500] + "\n\n" + self.chunks[1][:1500]
        elif len(self.chunks) == 1:
            important_content = self.chunks[0][:2000]
        else:
            important_content = "N·ªôi dung t√†i li·ªáu ng·∫Øn"
        
        prompt = f"""T√≥m t·∫Øt ng·∫Øn g·ªçn n·ªôi dung sau trong 100 t·ª´:

{important_content}

T√≥m t·∫Øt ch·ªß ƒë·ªÅ ch√≠nh v√† kh√°i ni·ªám quan tr·ªçng:"""
        
        summary = self.call_openai_api(prompt, max_tokens=300)
        if summary:
            self.document_summary = summary
            print("‚úÖ ƒê√£ t·∫°o t√≥m t·∫Øt t√†i li·ªáu")
        else:
            self.document_summary = "T√†i li·ªáu h·ªçc t·∫≠p v·ªõi c√°c kh√°i ni·ªám v√† ki·∫øn th·ª©c c·∫ßn thi·∫øt."
            print("‚ö†Ô∏è S·ª≠ d·ª•ng t√≥m t·∫Øt m·∫∑c ƒë·ªãnh")
        
        return self.document_summary
    
    def get_relevant_content_for_topic(self, topic, num_chunks=3):
        """L·∫•y n·ªôi dung li√™n quan cho m·ªôt ch·ªß ƒë·ªÅ c·ª• th·ªÉ"""
        if not self.chunks or len(self.embeddings) == 0:
            return ""
        
        # T·∫°o embedding cho topic
        topic_embedding = self.embedding_model.encode([topic])
        
        # T√≠nh similarity
        similarities = cosine_similarity(topic_embedding, self.embeddings)[0]
        
        # L·∫•y top chunks
        top_indices = np.argsort(similarities)[::-1][:num_chunks]
        
        relevant_content = []
        for idx in top_indices:
            relevant_content.append(self.chunks[idx])
        
        return "\n\n".join(relevant_content)
    
    def generate_questions(self, num_questions=5):
        """T·∫°o c√¢u h·ªèi tr·∫Øc nghi·ªám t·ª´ t√†i li·ªáu v·ªõi batch processing"""
        print(f"ü§ñ ƒêang t·∫°o {num_questions} c√¢u h·ªèi tr·∫Øc nghi·ªám...")
        
        # T·∫°o t√≥m t·∫Øt n·∫øu ch∆∞a c√≥
        if not self.document_summary:
            self.generate_document_summary()
        
        # Chia th√†nh batch ƒë·ªÉ tr√°nh gi·ªõi h·∫°n token
        batch_size = 8  # S·ªë c√¢u h·ªèi t·ªëi ƒëa m·ªói batch
        all_questions = {"questions": []}
        
        if num_questions <= batch_size:
            # N·∫øu s·ªë c√¢u h·ªèi nh·ªè, t·∫°o m·ªôt l·∫ßn
            return self.generate_questions_batch(num_questions, 1)
        
        # Chia th√†nh nhi·ªÅu batch
        total_batches = (num_questions + batch_size - 1) // batch_size
        print(f"üì¶ Chia th√†nh {total_batches} batch (t·ªëi ƒëa {batch_size} c√¢u/batch)")
        
        question_id = 1
        for batch_num in range(total_batches):
            remaining_questions = num_questions - len(all_questions["questions"])
            current_batch_size = min(batch_size, remaining_questions)
            
            if current_batch_size <= 0:
                break
                
            print(f"üîÑ ƒêang x·ª≠ l√Ω batch {batch_num + 1}/{total_batches} ({current_batch_size} c√¢u)...")
            
            # T·∫°o c√¢u h·ªèi cho batch n√†y
            batch_questions = self.generate_questions_batch(current_batch_size, question_id)
            
            if batch_questions and batch_questions.get("questions"):
                # C·∫≠p nh·∫≠t ID cho c√¢u h·ªèi
                for q in batch_questions["questions"]:
                    q["id"] = question_id
                    question_id += 1
                    all_questions["questions"].append(q)
                
                print(f"‚úÖ Ho√†n th√†nh batch {batch_num + 1} - {len(batch_questions['questions'])} c√¢u")
            else:
                print(f"‚ö†Ô∏è Batch {batch_num + 1} th·∫•t b·∫°i, th·ª≠ l·∫°i...")
                # Th·ª≠ l·∫°i v·ªõi format ƒë∆°n gi·∫£n
                batch_questions = self.generate_questions_simple_format_batch(current_batch_size, question_id)
                if batch_questions and batch_questions.get("questions"):
                    for q in batch_questions["questions"]:
                        all_questions["questions"].append(q)
                        question_id += 1
        
        print(f"üéØ T·ªïng c·ªông ƒë√£ t·∫°o {len(all_questions['questions'])}/{num_questions} c√¢u h·ªèi")
        return all_questions
    
    def get_concise_content(self, num_questions):
        """L·∫•y n·ªôi dung ng·∫Øn g·ªçn cho t·∫°o c√¢u h·ªèi"""
        if not self.chunks:
            return "N·ªôi dung t√†i li·ªáu c·∫ßn thi·∫øt cho c√¢u h·ªèi."
        
        # L·∫•y chunks ƒë·∫ßu v√† gi·ªØa, gi·ªõi h·∫°n ƒë·ªô d√†i
        content_parts = []
        
        if len(self.chunks) >= 2:
            # Chunk ƒë·∫ßu
            content_parts.append(self.chunks[0][:800])
            # Chunk gi·ªØa
            mid_idx = len(self.chunks) // 2
            content_parts.append(self.chunks[mid_idx][:800])
        else:
            content_parts.append(self.chunks[0][:1200])
        
        return "\n\n".join(content_parts)

    def generate_questions_batch(self, batch_size, start_id):
        """T·∫°o m·ªôt batch c√¢u h·ªèi v·ªõi prompt t·ªëi ∆∞u"""
        # L·∫•y n·ªôi dung ng·∫Øn g·ªçn
        content_summary = self.get_concise_content(batch_size)
        
        prompt = f"""T·∫°o {batch_size} c√¢u h·ªèi tr·∫Øc nghi·ªám t·ª´ n·ªôi dung sau:

KI·∫æN TH·ª®C:
{content_summary}

Y√äU C·∫¶U:
- {batch_size} c√¢u h·ªèi, m·ªói c√¢u 4 ƒë√°p √°n A,B,C,D
- Ki·ªÉm tra hi·ªÉu bi·∫øt, kh√¥ng h·ªèi "t√†i li·ªáu n√≥i g√¨"
- ƒê√°p √°n ƒë√∫ng duy nh·∫•t, sai h·ª£p l√Ω

FORMAT JSON:
{{
  "questions": [
    {{
      "question": "C√¢u h·ªèi...",
      "type": "multiple_choice",
      "hint": "G·ª£i √Ω ng·∫Øn",
      "correct_answer": "A",
      "options": [
        {{"answer": "A", "content": "ƒê√°p √°n A", "reason": "ƒê√∫ng v√¨..."}},
        {{"answer": "B", "content": "ƒê√°p √°n B", "reason": "Sai v√¨..."}},
        {{"answer": "C", "content": "ƒê√°p √°n C", "reason": "Sai v√¨..."}},
        {{"answer": "D", "content": "ƒê√°p √°n D", "reason": "Sai v√¨..."}}
      ]
    }}
  ]
}}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m:"""
        {{"answer": "B", "content": "ƒê√°p √°n B", "reason": "Sai v√¨..."}},
        {{"answer": "C", "content": "ƒê√°p √°n C", "reason": "Sai v√¨..."}},
        {{"answer": "D", "content": "ƒê√°p √°n D", "reason": "Sai v√¨..."}}
      ]
    }}
  ]
}}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m:"""

üé® M·∫™U C√ÇU H·ªéI T·ªêT:
"Khi √°p d·ª•ng ph∆∞∆°ng ph√°p X ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ Y, b∆∞·ªõc ƒë·∫ßu ti√™n quan tr·ªçng nh·∫•t l√† g√¨?"
A) Ph√¢n t√≠ch nguy√™n nh√¢n g·ªëc r·ªÖ
B) Thu th·∫≠p th√¥ng tin ban ƒë·∫ßu  
C) ƒê·ªÅ xu·∫•t gi·∫£i ph√°p ngay l·∫≠p t·ª©c
D) ƒê√°nh gi√° t√°c ƒë·ªông c√≥ th·ªÉ x·∫£y ra

üìã FORMAT JSON TR·∫¢ V·ªÄ:
{{
  "questions": [
    {{
      "id": {start_id},
      "question": "C√¢u h·ªèi ki·ªÉm tra ki·∫øn th·ª©c (KH√îNG h·ªèi v·ªÅ t√†i li·ªáu)?",
      "options": {{
        "A": "L·ª±a ch·ªçn A",
        "B": "L·ª±a ch·ªçn B", 
        "C": "L·ª±a ch·ªçn C",
        "D": "L·ª±a ch·ªçn D"
      }},
      "correct_answer": "B",
      "hint": "G·ª£i √Ω ng·∫Øn g·ªçn gi√∫p nh·ªõ l·∫°i ki·∫øn th·ª©c",
      "explanation": "Gi·∫£i th√≠ch chi ti·∫øt d·ª±a tr√™n ki·∫øn th·ª©c ƒë√£ h·ªçc",
      "difficulty": "easy|medium|hard",
      "topic": "Ch·ªß ƒë·ªÅ ki·∫øn th·ª©c li√™n quan"
    }}
  ]
}}

üöÄ B·∫ÆT ƒê·∫¶U T·∫†O {batch_size} C√ÇU H·ªéI √îN T·∫¨P:
"""
        
        result = self.call_openai_api(prompt, max_tokens=2500)
        
        try:
            questions_data = json.loads(result)
            return questions_data
        except json.JSONDecodeError:
            print("‚ö†Ô∏è L·ªói parse JSON trong batch")
            return None
    
    def generate_questions_simple_format_batch(self, batch_size, start_id):
        """T·∫°o batch c√¢u h·ªèi v·ªõi format ƒë∆°n gi·∫£n"""
        questions = {"questions": []}
        
        for i in range(batch_size):
            current_id = start_id + i
            
            # L·∫•y n·ªôi dung li√™n quan cho c√¢u h·ªèi
            topics = ["kh√°i ni·ªám ch√≠nh", "ƒë·ªãnh nghƒ©a", "·ª©ng d·ª•ng", "v√≠ d·ª•", "ph∆∞∆°ng ph√°p", "ƒë·∫∑c ƒëi·ªÉm", "nguy√™n l√Ω", "quy tr√¨nh"]
            topic = topics[i % len(topics)]
            
            relevant_content = self.get_relevant_content_for_topic(topic, 2)
            
            prompt = f"""
D·ª±a tr√™n n·ªôi dung sau, t·∫°o 1 c√¢u h·ªèi tr·∫Øc nghi·ªám ch·∫•t l∆∞·ª£ng cao:

N·ªòI DUNG:
{relevant_content[:1500]}

Tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng:
QUESTION: [c√¢u h·ªèi r√µ r√†ng v√† c·ª• th·ªÉ]
A: [l·ª±a ch·ªçn A]
B: [l·ª±a ch·ªçn B]
C: [l·ª±a ch·ªçn C] 
D: [l·ª±a ch·ªçn D]
ANSWER: [A/B/C/D]
HINT: [g·ª£i √Ω h·ªØu √≠ch]
EXPLANATION: [l√Ω do t·∫°i sao ƒë√°p √°n n√†y ƒë√∫ng]
"""
            
            result = self.call_openai_api(prompt, max_tokens=800)
            
            # Parse k·∫øt qu·∫£
            question_data = self.parse_simple_question(result, current_id)
            if question_data:
                questions["questions"].append(question_data)
            else:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ parse c√¢u h·ªèi {current_id}")
        
        return questions

    def get_diverse_content_for_questions(self, num_questions):
        """L·∫•y n·ªôi dung ƒëa d·∫°ng t·ª´ c√°c ph·∫ßn kh√°c nhau c·ªßa t√†i li·ªáu"""
        if not self.chunks:
            return self.clean_text(self.pdf_content)[:3000]
        
        # Chia chunks th√†nh c√°c nh√≥m
        total_chunks = len(self.chunks)
        
        # ƒê·∫£m b·∫£o l·∫•y ƒë·ªß n·ªôi dung ƒëa d·∫°ng
        if total_chunks <= num_questions:
            # N·∫øu √≠t chunks, l·∫•y t·∫•t c·∫£
            selected_content = self.chunks
        else:
            # Chia ƒë·ªÅu chunks ƒë·ªÉ l·∫•y n·ªôi dung t·ª´ c√°c ph·∫ßn kh√°c nhau
            step = total_chunks // num_questions
            selected_content = []
            for i in range(0, total_chunks, max(1, step)):
                if len(selected_content) < num_questions and i < total_chunks:
                    selected_content.append(self.chunks[i])
        
        return "\n\n".join(selected_content)
    
    def generate_questions_simple_format(self, num_questions=5):
        """T·∫°o c√¢u h·ªèi v·ªõi format ƒë∆°n gi·∫£n h∆°n"""
        questions = {"questions": []}
        
        for i in range(num_questions):
            # L·∫•y n·ªôi dung li√™n quan cho c√¢u h·ªèi
            topics = ["kh√°i ni·ªám ch√≠nh", "ƒë·ªãnh nghƒ©a", "·ª©ng d·ª•ng", "v√≠ d·ª•", "ph∆∞∆°ng ph√°p"]
            topic = topics[i % len(topics)]
            
            relevant_content = self.get_relevant_content_for_topic(topic, 2)
            
            prompt = f"""
D·ª±a tr√™n n·ªôi dung sau, t·∫°o 1 c√¢u h·ªèi tr·∫Øc nghi·ªám:

N·ªòI DUNG:
{relevant_content[:1500]}

Tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng:
QUESTION: [c√¢u h·ªèi]
A: [l·ª±a ch·ªçn A]
B: [l·ª±a ch·ªçn B]
C: [l·ª±a ch·ªçn C] 
D: [l·ª±a ch·ªçn D]
ANSWER: [A/B/C/D]
HINT: [g·ª£i √Ω]
EXPLANATION: [l√Ω do]
"""
            
            result = self.call_openai_api(prompt, max_tokens=800)
            
            # Parse k·∫øt qu·∫£
            question_data = self.parse_simple_question(result, i + 1)
            if question_data:
                questions["questions"].append(question_data)
        
        return questions
    
    def parse_simple_question(self, text, question_id):
        """Parse c√¢u h·ªèi t·ª´ format ƒë∆°n gi·∫£n"""
        try:
            lines = text.strip().split('\n')
            question_data = {"id": question_id, "options": {}}
            
            for line in lines:
                line = line.strip()
                if line.startswith('QUESTION:'):
                    question_data["question"] = line.replace('QUESTION:', '').strip()
                elif line.startswith('A:'):
                    question_data["options"]["A"] = line.replace('A:', '').strip()
                elif line.startswith('B:'):
                    question_data["options"]["B"] = line.replace('B:', '').strip()
                elif line.startswith('C:'):
                    question_data["options"]["C"] = line.replace('C:', '').strip()
                elif line.startswith('D:'):
                    question_data["options"]["D"] = line.replace('D:', '').strip()
                elif line.startswith('ANSWER:'):
                    question_data["correct_answer"] = line.replace('ANSWER:', '').strip()
                elif line.startswith('HINT:'):
                    question_data["hint"] = line.replace('HINT:', '').strip()
                elif line.startswith('EXPLANATION:'):
                    question_data["explanation"] = line.replace('EXPLANATION:', '').strip()
            
            return question_data
        except:
            return None
    
    def save_questions(self, questions_data, filename=None):
        """L∆∞u c√¢u h·ªèi ra file"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"questions_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(questions_data, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ ƒê√£ l∆∞u c√¢u h·ªèi v√†o: {filename}")
            return filename
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u file: {str(e)}")
            return None
    
    def display_questions(self, questions_data):
        """Hi·ªÉn th·ªã c√¢u h·ªèi ra m√†n h√¨nh"""
        print("\n" + "="*80)
        print("üìã C√ÇU H·ªéI TR·∫ÆC NGHI·ªÜM ƒê√É T·∫†O")
        print("="*80)
        
        for q in questions_data.get("questions", []):
            print(f"\nüî¢ C√¢u {q.get('id', '?')}: {q.get('question', 'N/A')}")
            print("-" * 60)
            
            options = q.get('options', {})
            for key in ['A', 'B', 'C', 'D']:
                if key in options:
                    print(f"   {key}. {options[key]}")
            
            print(f"\n‚úÖ ƒê√°p √°n ƒë√∫ng: {q.get('correct_answer', 'N/A')}")
            print(f"üí° G·ª£i √Ω: {q.get('hint', 'Kh√¥ng c√≥')}")
            print(f"üìù Gi·∫£i th√≠ch: {q.get('explanation', 'Kh√¥ng c√≥')}")
            print("-" * 60)
    
    def process_pdf(self, pdf_path, num_questions=5):
        """Quy tr√¨nh ho√†n ch·ªânh: PDF ‚Üí C√¢u h·ªèi"""
        print(f"üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù: {Path(pdf_path).name}")
        print("="*60)
        
        # B∆∞·ªõc 1: Chuy·ªÉn PDF sang text
        if not self.convert_pdf_to_text(pdf_path):
            return None
        
        # B∆∞·ªõc 2: T·∫°o embeddings ƒë·ªÉ hi·ªÉu t√†i li·ªáu
        if not self.create_embeddings():
            return None
        
        # B∆∞·ªõc 3: T·∫°o c√¢u h·ªèi
        questions_data = self.generate_questions(num_questions)
        
        if questions_data and questions_data.get("questions"):
            # B∆∞·ªõc 4: Hi·ªÉn th·ªã k·∫øt qu·∫£
            self.display_questions(questions_data)
            
            # B∆∞·ªõc 5: L∆∞u file
            filename = self.save_questions(questions_data)
            
            print(f"\nüéâ HO√ÄN TH√ÄNH!")
            print(f"üìä ƒê√£ t·∫°o {len(questions_data['questions'])} c√¢u h·ªèi")
            print(f"üíæ File k·∫øt qu·∫£: {filename}")
            
            return questions_data
        else:
            print("‚ùå Kh√¥ng th·ªÉ t·∫°o c√¢u h·ªèi")
            return None

    def generate_questions_batch_optimized(self, num_questions=5):
        """T·∫°o c√¢u h·ªèi v·ªõi batch processing t·ªëi ∆∞u cho t·ªëc ƒë·ªô"""
        print(f"üöÄ T·∫°o {num_questions} c√¢u h·ªèi (OPTIMIZED)...")
        
        # T·∫°o t√≥m t·∫Øt n·∫øu ch∆∞a c√≥
        if not self.document_summary:
            self.generate_document_summary()
        
        # T·ªëi ∆∞u batch size d·ª±a tr√™n s·ªë c√¢u h·ªèi
        if num_questions <= 10:
            batch_size = num_questions  # T·∫°o m·ªôt l·∫ßn
        elif num_questions <= 50:
            batch_size = 10  # Batch 10 c√¢u
        else:
            batch_size = 15  # Batch 15 c√¢u cho s·ªë l∆∞·ª£ng l·ªõn
        
        all_questions = []
        total_batches = (num_questions + batch_size - 1) // batch_size
        
        print(f"üì¶ X·ª≠ l√Ω {total_batches} batch (t·ªëi ƒëa {batch_size} c√¢u/batch)")
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, num_questions)
            questions_in_batch = end_idx - start_idx
            
            print(f"‚ö° Batch {batch_num + 1}/{total_batches}: {questions_in_batch} c√¢u")
            
            # T·∫°o prompt t·ªëi ∆∞u cho batch
            prompt = self.create_optimized_batch_prompt(questions_in_batch, batch_num)
            
            try:
                # G·ªçi API v·ªõi timeout ng·∫Øn h∆°n
                response = self.call_openai_api_optimized(prompt, max_tokens=3000)
                
                # Parse response
                batch_questions = self.parse_questions_response_fast(response)
                
                if batch_questions:
                    all_questions.extend(batch_questions)
                    print(f"‚úÖ Batch {batch_num + 1}: +{len(batch_questions)} c√¢u")
                else:
                    print(f"‚ö†Ô∏è Batch {batch_num + 1}: Kh√¥ng c√≥ c√¢u h·ªèi h·ª£p l·ªá")
                    
            except Exception as e:
                print(f"‚ùå L·ªói batch {batch_num + 1}: {str(e)}")
                continue
        
        print(f"üéØ Ho√†n th√†nh: {len(all_questions)}/{num_questions} c√¢u h·ªèi")
        return all_questions[:num_questions]  # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° y√™u c·∫ßu

    def create_optimized_batch_prompt(self, num_questions, batch_num):
        """T·∫°o prompt t·ªëi ∆∞u cho batch processing"""
        # L·∫•y content relevant cho batch n√†y
        relevant_content = self.get_diverse_content_for_batch(batch_num)
        
        prompt = f"""D·ª±a tr√™n n·ªôi dung t√†i li·ªáu sau, h√£y t·∫°o CH√çNH X√ÅC {num_questions} c√¢u h·ªèi tr·∫Øc nghi·ªám ch·∫•t l∆∞·ª£ng cao.

N·ªòI DUNG THAM KH·∫¢O:
{self.document_summary}

CHI TI·∫æT:
{relevant_content}

Y√äU C·∫¶U:
- T·∫°o ƒê√öNG {num_questions} c√¢u h·ªèi (kh√¥ng √≠t h∆°n, kh√¥ng nhi·ªÅu h∆°n)
- M·ªói c√¢u h·ªèi c√≥ 4 l·ª±a ch·ªçn A, B, C, D
- ƒê√°p √°n ch√≠nh x√°c v√† gi·∫£i th√≠ch r√µ r√†ng
- C√¢u h·ªèi ƒëa d·∫°ng v·ªÅ m·ª©c ƒë·ªô (d·ªÖ, trung b√¨nh, kh√≥)
- T·∫≠p trung v√†o ki·∫øn th·ª©c c·ªët l√µi

FORMAT JSON (QUAN TR·ªåNG):
[
    {{
        "question": "C√¢u h·ªèi 1...",
        "type": "multiple_choice",
        "hint": "G·ª£i √Ω...",
        "correct_answer": "A",
        "options": [
            {{"answer": "A", "reason": "ƒê√°p √°n A v√¨..."}},
            {{"answer": "B", "reason": "ƒê√°p √°n B sai v√¨..."}},
            {{"answer": "C", "reason": "ƒê√°p √°n C sai v√¨..."}},
            {{"answer": "D", "reason": "ƒê√°p √°n D sai v√¨..."}}
        ]
    }}
]

Ch·ªâ tr·∫£ v·ªÅ JSON array, kh√¥ng gi·∫£i th√≠ch th√™m."""
        
        return prompt

    def get_diverse_content_for_batch(self, batch_num):
        """L·∫•y n·ªôi dung ƒëa d·∫°ng cho m·ªói batch"""
        if not self.chunks:
            return ""
        
        # T√≠nh offset ƒë·ªÉ m·ªói batch c√≥ n·ªôi dung kh√°c nhau
        chunks_per_batch = max(3, len(self.chunks) // 4)
        start_idx = (batch_num * chunks_per_batch) % len(self.chunks)
        
        # L·∫•y chunks v·ªõi rotation ƒë·ªÉ ƒë·∫£m b·∫£o ƒëa d·∫°ng
        selected_chunks = []
        for i in range(chunks_per_batch):
            chunk_idx = (start_idx + i) % len(self.chunks)
            selected_chunks.append(self.chunks[chunk_idx])
        
        return "\n\n---\n\n".join(selected_chunks)

    def call_openai_api_optimized(self, prompt, max_tokens=3000):
        """G·ªçi OpenAI API v·ªõi t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô"""
        try:
            import requests
            import json
            
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self.openai_api_key}'
            }
            
            data = {
                'model': 'gpt-3.5-turbo',  # S·ª≠ d·ª•ng model nhanh h∆°n
                'messages': [
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ],
                'max_tokens': max_tokens,
                'temperature': 0.7,
                'timeout': 30  # Timeout ng·∫Øn
            }
            
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=30  # Request timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                print(f"‚ùå OpenAI API error: {response.status_code}")
                print(f"Response: {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            print("‚è∞ OpenAI API timeout")
            return None
        except Exception as e:
            print(f"‚ùå L·ªói g·ªçi OpenAI API: {str(e)}")
            return None

    def parse_questions_response_fast(self, response):
        """Parse response nhanh v·ªõi error handling t·ªët"""
        if not response:
            return []
        
        try:
            # T√¨m JSON trong response
            import re
            
            # T√¨m JSON array pattern
            json_pattern = r'\[[\s\S]*\]'
            json_match = re.search(json_pattern, response)
            
            if json_match:
                json_str = json_match.group(0)
                questions = json.loads(json_str)
                
                # Validate questions format
                valid_questions = []
                for q in questions:
                    if self.validate_question_format_fast(q):
                        valid_questions.append(q)
                
                return valid_questions
            else:
                # Fallback: t√¨m c√°ch parse kh√°c
                return self.parse_questions_fallback(response)
                
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è L·ªói parse JSON: {str(e)}")
            return self.parse_questions_fallback(response)
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói parse response: {str(e)}")
            return []

    def validate_question_format_fast(self, question):
        """Validate format c√¢u h·ªèi nhanh"""
        try:
            required_fields = ['question', 'type', 'hint', 'correct_answer', 'options']
            
            # Ki·ªÉm tra c√°c field b·∫Øt bu·ªôc
            for field in required_fields:
                if field not in question:
                    return False
            
            # Ki·ªÉm tra options
            options = question.get('options', [])
            if len(options) != 4:
                return False
            
            # Ki·ªÉm tra format options
            for option in options:
                if not isinstance(option, dict):
                    return False
                if 'answer' not in option or 'reason' not in option:
                    return False
            
            return True
            
        except Exception:
            return False

    def parse_questions_fallback(self, response):
        """Fallback parsing cho tr∆∞·ªùng h·ª£p JSON kh√¥ng chu·∫©n"""
        try:
            # Implement simple parsing logic
            questions = []
            # Add simple regex-based parsing if needed
            print("üîÑ S·ª≠ d·ª•ng fallback parsing...")
            return questions
        except Exception:
            return []
    
def main():
    """H√†m ch√≠nh"""
    generator = PDFQuestionGenerator()
    
    # T√¨m file PDF
    pdf_files = list(Path(".").glob("*.pdf"))
    
    if pdf_files:
        print("üìã File PDF t√¨m th·∫•y:")
        for i, pdf_file in enumerate(pdf_files, 1):
            file_size = os.path.getsize(pdf_file) / 1024
            print(f"   {i}. {pdf_file.name} ({file_size:.1f} KB)")
        
        try:
            choice = input(f"\nüî¢ Ch·ªçn file (1-{len(pdf_files)}) ho·∫∑c nh·∫≠p ƒë∆∞·ªùng d·∫´n: ").strip()
            
            if choice.isdigit() and 1 <= int(choice) <= len(pdf_files):
                pdf_path = str(pdf_files[int(choice) - 1])
            else:
                pdf_path = choice.strip('"').strip("'")
                
        except (ValueError, KeyboardInterrupt):
            print("\nüëã ƒê√£ h·ªßy!")
            return
    else:
        pdf_path = input("üìÅ Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn file PDF: ").strip('"').strip("'")
    
    if not pdf_path or not os.path.exists(pdf_path):
        print("‚ùå File PDF kh√¥ng t·ªìn t·∫°i!")
        return
    
    # Nh·∫≠p s·ªë c√¢u h·ªèi
    try:
        num_questions = int(input("\nüî¢ S·ªë c√¢u h·ªèi mu·ªën t·∫°o (m·∫∑c ƒë·ªãnh 5): ") or "5")
        if num_questions < 1:
            print("‚ö†Ô∏è S·ªë c√¢u h·ªèi ph·∫£i l·ªõn h∆°n 0, ƒë·∫∑t v·ªÅ m·∫∑c ƒë·ªãnh")
            num_questions = 5
        elif num_questions > 200:
            print("‚ö†Ô∏è S·ªë c√¢u h·ªèi qu√° l·ªõn, gi·ªõi h·∫°n t·ªëi ƒëa 200 c√¢u")
            num_questions = 200
    except ValueError:
        print("‚ö†Ô∏è S·ªë kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh")
        num_questions = 5
    
    # X·ª≠ l√Ω PDF v√† t·∫°o c√¢u h·ªèi
    result = generator.process_pdf(pdf_path, num_questions)
    
    if result:
        print("\n‚úÖ Qu√° tr√¨nh ho√†n t·∫•t th√†nh c√¥ng!")
    else:
        print("\n‚ùå Qu√° tr√¨nh th·∫•t b·∫°i!")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nüëã ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng!")
    except Exception as e:
        print(f"\n‚ùå L·ªói nghi√™m tr·ªçng: {str(e)}")
        sys.exit(1)