import os
import json
import requests
from pathlib import Path
from dotenv import load_dotenv
import subprocess
import sys
import hashlib
from datetime import datetime

# Import cho RAG
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle
except ImportError:
    print("‚ùå Thi·∫øu m·ªôt s·ªë th∆∞ vi·ªán c·∫ßn thi·∫øt. ƒêang c√†i ƒë·∫∑t...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers", "scikit-learn", "numpy"])
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle

# Load environment variables
load_dotenv()

class PDFChatRAG:
    def __init__(self):
        """Kh·ªüi t·∫°o h·ªá th·ªëng RAG cho PDF chat"""
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y OPENAI_API_KEY trong file .env")
            sys.exit(1)
        
        # Kh·ªüi t·∫°o model embedding
        print("üîÑ ƒêang t·∫£i m√¥ h√¨nh embedding...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Bi·∫øn l∆∞u tr·ªØ
        self.chunks = []
        self.embeddings = []
        self.pdf_content = ""
        
        # TH√äM: Qu·∫£n l√Ω cache v√† l·ªãch s·ª≠
        self.current_pdf_path = None
        self.conversation_history = []
        self.max_history = 10
        
        print("‚úÖ H·ªá th·ªëng RAG ƒë√£ s·∫µn s√†ng!")
    
    def convert_pdf_to_text(self, pdf_path):
        """Chuy·ªÉn ƒë·ªïi PDF sang text b·∫±ng pdfToText.py c·∫£i ti·∫øn"""
        try:
            print(f"üìÑ ƒêang chuy·ªÉn ƒë·ªïi PDF: {pdf_path}")
            
            # Import v√† s·ª≠ d·ª•ng function t·ª´ pdfToText.py
            import importlib.util
            spec = importlib.util.spec_from_file_location("pdfToText", "pdfToText.py")
            pdf_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(pdf_module)
            
            # Chuy·ªÉn ƒë·ªïi PDF v·ªõi error handling n√¢ng cao
            temp_output = f"temp_pdf_content_{hashlib.md5(pdf_path.encode()).hexdigest()[:8]}.txt"
            
            print("üîß S·ª≠ d·ª•ng PDF converter c·∫£i ti·∫øn...")
            success = pdf_module.pdf_to_text(pdf_path, temp_output)
            
            if success and os.path.exists(temp_output):
                with open(temp_output, 'r', encoding='utf-8') as f:
                    self.pdf_content = f.read()
                
                # X√≥a file t·∫°m
                os.remove(temp_output)
                
                # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng n·ªôi dung
                if len(self.pdf_content.strip()) == 0:
                    print("‚ö†Ô∏è C·∫£nh b√°o: PDF kh√¥ng ch·ª©a text c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c")
                    return False
                
                # ƒê·∫øm c√°c trang l·ªói
                error_pages = self.pdf_content.count("[L·ªñI TR√çCH XU·∫§T:")
                empty_pages = self.pdf_content.count("[TRANG TR·ªêNG HO·∫∂C KH√îNG C√ì TEXT]")
                
                if error_pages > 0:
                    print(f"‚ö†Ô∏è C√≥ {error_pages} trang b·ªã l·ªói khi tr√≠ch xu·∫•t")
                if empty_pages > 0:
                    print(f"‚ÑπÔ∏è C√≥ {empty_pages} trang tr·ªëng")
                
                print("‚úÖ Chuy·ªÉn ƒë·ªïi PDF th√†nh c√¥ng!")
                return True
            else:
                print("‚ùå L·ªói chuy·ªÉn ƒë·ªïi PDF")
                return False
                
        except ImportError as e:
            print(f"‚ùå L·ªói import pdfToText.py: {str(e)}")
            print("üí° ƒê·∫£m b·∫£o file pdfToText.py c√≥ trong th∆∞ m·ª•c n√†y")
            return False
        except Exception as e:
            print(f"‚ùå L·ªói: {str(e)}")
            return False
    
    def create_chunks(self, text, chunk_size=500, overlap=50):
        """Chia text th√†nh c√°c chunks nh·ªè v·ªõi overlap v√† l√†m s·∫°ch d·ªØ li·ªáu"""
        # L√†m s·∫°ch text tr∆∞·ªõc khi chia chunks
        cleaned_text = self.clean_text(text)
        
        words = cleaned_text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            
            # Ch·ªâ th√™m chunk n·∫øu c√≥ n·ªôi dung h·ªØu √≠ch
            if len(chunk.strip()) > 50 and not self.is_error_chunk(chunk):
                chunks.append(chunk.strip())
        
        print(f"üìö ƒê√£ t·∫°o {len(chunks)} chunks h·ªØu √≠ch t·ª´ t√†i li·ªáu")
        return chunks
    
    def clean_text(self, text):
        """L√†m s·∫°ch text v√† lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt"""
        import re
        
        # Lo·∫°i b·ªè header trang
        text = re.sub(r'={50}\nTRANG \d+\n={50}', '', text)
        
        # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def is_error_chunk(self, chunk):
        """Ki·ªÉm tra xem chunk c√≥ ph·∫£i l√† l·ªói kh√¥ng"""
        error_indicators = [
            "[L·ªñI TR√çCH XU·∫§T:",
            "[TRANG TR·ªêNG HO·∫∂C KH√îNG C√ì TEXT]",
            "[TRANG", ": KH√îNG TH·ªÇ TR√çCH XU·∫§T TEXT]"
        ]
        
        return any(indicator in chunk for indicator in error_indicators)
    
    def create_embeddings(self):
        """T·∫°o embeddings cho c√°c chunks"""
        if not self.pdf_content:
            print("‚ùå Ch∆∞a c√≥ n·ªôi dung PDF ƒë·ªÉ x·ª≠ l√Ω")
            return False
        
        print("üîÑ ƒêang t·∫°o chunks v√† embeddings...")
        
        # T·∫°o chunks
        self.chunks = self.create_chunks(self.pdf_content)
        
        # T·∫°o embeddings
        self.embeddings = self.embedding_model.encode(self.chunks)
        
        print(f"‚úÖ ƒê√£ t·∫°o embeddings cho {len(self.chunks)} chunks")
        
        # L∆∞u cache v·ªõi t√™n file ri√™ng
        if self.current_pdf_path:
            self.save_cache()
        
        return True
    
    def save_cache(self):
        """L∆∞u cache v·ªõi t√™n file ri√™ng"""
        try:
            cache_filename = self.get_cache_filename(self.current_pdf_path)
            
            cache_data = {
                'pdf_path': self.current_pdf_path,
                'pdf_modified_time': os.path.getmtime(self.current_pdf_path),
                'chunks': self.chunks,
                'embeddings': self.embeddings.tolist(),
                'pdf_content': self.pdf_content
            }
            
            with open(cache_filename, 'wb') as f:
                pickle.dump(cache_data, f)
            
            print(f"üíæ ƒê√£ l∆∞u cache: {cache_filename}")
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u cache: {str(e)}")
            return False
    
    def load_cache(self, pdf_path):
        """T·∫£i cache cho PDF c·ª• th·ªÉ"""
        try:
            cache_filename = self.get_cache_filename(pdf_path)
            
            if not os.path.exists(cache_filename):
                print(f"‚ÑπÔ∏è Ch∆∞a c√≥ cache cho PDF: {Path(pdf_path).name}")
                return False
            
            with open(cache_filename, 'rb') as f:
                cache_data = pickle.load(f)
            
            # Ki·ªÉm tra PDF c√≥ thay ƒë·ªïi kh√¥ng
            if os.path.exists(pdf_path):
                current_modified_time = os.path.getmtime(pdf_path)
                cached_modified_time = cache_data.get('pdf_modified_time', 0)
                
                if abs(current_modified_time - cached_modified_time) > 1:
                    print("‚ö†Ô∏è PDF ƒë√£ thay ƒë·ªïi, cache kh√¥ng c√≤n h·ª£p l·ªá")
                    return False
            
            # T·∫£i d·ªØ li·ªáu t·ª´ cache
            self.chunks = cache_data['chunks']
            self.embeddings = np.array(cache_data['embeddings'])
            self.pdf_content = cache_data['pdf_content']
            self.current_pdf_path = pdf_path
            
            print(f"‚úÖ ƒê√£ t·∫£i cache: {cache_filename}")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i cache: {str(e)}")
            return False
    
    def retrieve_relevant_chunks(self, query, top_k=3):
        """T√¨m ki·∫øm chunks li√™n quan nh·∫•t v·ªõi c√¢u h·ªèi"""
        if not self.chunks:
            return []
        
        # T·∫°o embedding cho query
        query_embedding = self.embedding_model.encode([query])
        
        # T√≠nh cosine similarity
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # L·∫•y top_k chunks c√≥ similarity cao nh·∫•t
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        relevant_chunks = []
        for idx in top_indices:
            relevant_chunks.append({
                'text': self.chunks[idx],
                'similarity': similarities[idx]
            })
        
        return relevant_chunks
    
    def call_openai_api(self, prompt):
        """G·ªçi OpenAI API"""
        try:
            url = "https://api.openai.com/v1/chat/completions"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.openai_api_key}"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.7
            }
            
            response = requests.post(url, headers=headers, json=data)
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                return "‚ùå L·ªói: Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ OpenAI"
                
        except Exception as e:
            return f"‚ùå L·ªói g·ªçi API: {str(e)}"
    
    def generate_answer(self, question):
        """T·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n RAG v·ªõi context t·ª´ l·ªãch s·ª≠"""
        print(f"üîç ƒêang t√¨m ki·∫øm th√¥ng tin li√™n quan...")
        
        # Retrieve relevant chunks
        relevant_chunks = self.retrieve_relevant_chunks(question, top_k=3)
        
        if not relevant_chunks:
            return "‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu", []
        
        # T·∫°o context t·ª´ relevant chunks
        document_context = "\n\n".join([chunk['text'] for chunk in relevant_chunks])
        
        # L·∫•y context t·ª´ l·ªãch s·ª≠ tr√≤ chuy·ªán
        conversation_context = self.get_conversation_context(question)
        
        # T·∫°o prompt v·ªõi context ƒë·∫ßy ƒë·ªß
        prompt = f"""
{conversation_context}

TH√îNG TIN T√ÄI LI·ªÜU LI√äN QUAN:
{document_context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi hi·ªán t·∫°i d·ª±a tr√™n:
1. Th√¥ng tin trong t√†i li·ªáu
2. B·ªëi c·∫£nh t·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥)

N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc, h√£y tham chi·∫øu ƒë·∫øn n√≥ m·ªôt c√°ch t·ª± nhi√™n.

TR·∫¢ L·ªúI:
"""
        
        print("ü§ñ ƒêang t·∫°o c√¢u tr·∫£ l·ªùi...")
        answer = self.call_openai_api(prompt)
        
        return answer, relevant_chunks
    
    def generate_extended_knowledge(self, question, answer):
        """T·∫°o ki·∫øn th·ª©c m·ªü r·ªông t·ª´ ngu·ªìn b√™n ngo√†i"""
        prompt = f"""
C√¢u h·ªèi g·ªëc: {question}
C√¢u tr·∫£ l·ªùi t·ª´ t√†i li·ªáu: {answer}

H√£y cung c·∫•p th√™m ki·∫øn th·ª©c m·ªü r·ªông, th√¥ng tin b·ªï sung, ho·∫∑c c√°c kh√≠a c·∫°nh li√™n quan kh√°c v·ªÅ ch·ªß ƒë·ªÅ n√†y t·ª´ ki·∫øn th·ª©c t·ªïng qu√°t c·ªßa b·∫°n. ƒê·ª´ng l·∫∑p l·∫°i th√¥ng tin ƒë√£ c√≥ trong c√¢u tr·∫£ l·ªùi g·ªëc.

KI·∫æN TH·ª®C M·ªû R·ªòNG:
"""
        
        print("üåê ƒêang t·∫°o ki·∫øn th·ª©c m·ªü r·ªông...")
        extended_knowledge = self.call_openai_api(prompt)
        
        return extended_knowledge
    
    def get_cache_filename(self, pdf_path):
        """T·∫°o t√™n cache ri√™ng cho t·ª´ng PDF"""
        # L·∫•y t√™n file PDF
        pdf_name = Path(pdf_path).stem
        
        # T·∫°o hash t·ª´ ƒë∆∞·ªùng d·∫´n ƒë·ªÉ tr√°nh tr√πng t√™n
        path_hash = hashlib.md5(pdf_path.encode()).hexdigest()[:8]
        
        return f"cache_{pdf_name}_{path_hash}.pkl"
    
    def add_to_history(self, question, answer, extended_knowledge=None):
        """Th√™m c√¢u h·ªèi v√† tr·∫£ l·ªùi v√†o l·ªãch s·ª≠"""
        conversation_item = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'question': question,
            'answer': answer,
            'extended_knowledge': extended_knowledge
        }
        
        self.conversation_history.append(conversation_item)
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i l·ªãch s·ª≠
        if len(self.conversation_history) > self.max_history:
            self.conversation_history.pop(0)
    
    def get_conversation_context(self, current_question):
        """L·∫•y context t·ª´ l·ªãch s·ª≠ tr√≤ chuy·ªán"""
        if not self.conversation_history:
            return ""
        
        context = "\n=== L·ªäCH S·ª¨ TR√í CHUY·ªÜN G·∫¶N ƒê√ÇY ===\n"
        
        # L·∫•y 3 cu·ªôc tr√≤ chuy·ªán g·∫ßn nh·∫•t
        recent_conversations = self.conversation_history[-3:]
        
        for i, conv in enumerate(recent_conversations, 1):
            context += f"\nC√¢u h·ªèi {i}: {conv['question']}\n"
            context += f"Tr·∫£ l·ªùi {i}: {conv['answer'][:200]}...\n"  # C·∫Øt ng·∫Øn ƒë·ªÉ ti·∫øt ki·ªám token
        
        context += f"\n=== C√ÇU H·ªéI HI·ªÜN T·∫†I ===\n{current_question}\n"
        
        return context
    
    def save_conversation_history(self, filename="conversation_history.json"):
        """L∆∞u l·ªãch s·ª≠ tr√≤ chuy·ªán ra file"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, indent=2, ensure_ascii=False)
            print(f"üíæ ƒê√£ l∆∞u l·ªãch s·ª≠ tr√≤ chuy·ªán v√†o {filename}")
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u l·ªãch s·ª≠: {str(e)}")
    
    def load_conversation_history(self, filename="conversation_history.json"):
        """T·∫£i l·ªãch s·ª≠ tr√≤ chuy·ªán t·ª´ file"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    self.conversation_history = json.load(f)
                print(f"üìö ƒê√£ t·∫£i {len(self.conversation_history)} cu·ªôc tr√≤ chuy·ªán t·ª´ l·ªãch s·ª≠")
                return True
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói t·∫£i l·ªãch s·ª≠: {str(e)}")
        return False
    
    def show_conversation_history(self):
        """Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán"""
        if not self.conversation_history:
            print("üìù Ch∆∞a c√≥ l·ªãch s·ª≠ tr√≤ chuy·ªán")
            return
        
        print(f"\nüìö L·ªäCH S·ª¨ TR√í CHUY·ªÜN ({len(self.conversation_history)} cu·ªôc tr√≤ chuy·ªán):")
        print("="*60)
        
        for i, conv in enumerate(self.conversation_history, 1):
            print(f"\nüïê {conv['timestamp']}")
            print(f"‚ùì C√¢u h·ªèi {i}: {conv['question']}")
            print(f"üí¨ Tr·∫£ l·ªùi: {conv['answer'][:150]}...")
            if conv.get('extended_knowledge'):
                print(f"üåê M·ªü r·ªông: {conv['extended_knowledge'][:100]}...")
            print("-" * 40)
    
    def list_cache_files(self):
        """Li·ªát k√™ c√°c file cache c√≥ s·∫µn"""
        cache_files = list(Path(".").glob("cache_*.pkl"))
        
        if not cache_files:
            print("üìù Ch∆∞a c√≥ cache n√†o")
            return []
        
        print("üìö Cache c√≥ s·∫µn:")
        cache_info = []
        
        for cache_file in cache_files:
            try:
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                
                pdf_name = Path(cache_data.get('pdf_path', 'Unknown')).name
                modified_time = cache_data.get('pdf_modified_time', 0)
                
                cache_info.append({
                    'file': cache_file,
                    'pdf_path': cache_data.get('pdf_path', ''),
                    'pdf_name': pdf_name
                })
                
                print(f"   üìÑ {pdf_name}")
                print(f"      Cache: {cache_file.name}")
                print(f"      Size: {cache_file.stat().st_size / 1024:.1f} KB")
                if modified_time:
                    print(f"      Th·ªùi gian: {datetime.fromtimestamp(modified_time)}")
                
            except Exception:
                print(f"   ‚ùå Cache l·ªói: {cache_file.name}")
        
        return cache_info
    
    def chat(self):
        """B·∫Øt ƒë·∫ßu chat loop v·ªõi l·ªãch s·ª≠ tr√≤ chuy·ªán"""
        # T·∫£i l·ªãch s·ª≠ tr√≤ chuy·ªán n·∫øu c√≥
        self.load_conversation_history()
        
        print("\n" + "="*60)
        print("ü§ñ PDF CHAT BOT v·ªõi RAG & L·ªäCH S·ª¨ TR√í CHUY·ªÜN")
        print("="*60)
        print("üí° Nh·∫≠p 'quit' ƒë·ªÉ tho√°t")
        print("üí° Nh·∫≠p 'info' ƒë·ªÉ xem th√¥ng tin t√†i li·ªáu")
        print("üí° Nh·∫≠p 'history' ƒë·ªÉ xem l·ªãch s·ª≠ tr√≤ chuy·ªán")
        print("üí° Nh·∫≠p 'clear' ƒë·ªÉ x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán")
        print("üí° Nh·∫≠p 'cache' ƒë·ªÉ xem c√°c file cache c√≥ s·∫µn")
        print("-"*60)
        
        while True:
            try:
                question = input("\n‚ùì C√¢u h·ªèi c·ªßa b·∫°n: ").strip()
                
                if question.lower() in ['quit', 'exit', 'q']:
                    # L∆∞u l·ªãch s·ª≠ tr∆∞·ªõc khi tho√°t
                    self.save_conversation_history()
                    print("üëã T·∫°m bi·ªát!")
                    break
                
                if question.lower() == 'info':
                    print(f"üìä Th√¥ng tin t√†i li·ªáu:")
                    print(f"   - S·ªë chunks: {len(self.chunks)}")
                    print(f"   - ƒê·ªô d√†i n·ªôi dung: {len(self.pdf_content):,} k√Ω t·ª±")
                    print(f"   - S·ªë cu·ªôc tr√≤ chuy·ªán: {len(self.conversation_history)}")
                    if self.current_pdf_path:
                        print(f"   - File PDF: {Path(self.current_pdf_path).name}")
                        
                        # Th√™m th·ªëng k√™ ch·∫•t l∆∞·ª£ng
                        error_pages = self.pdf_content.count("[L·ªñI TR√çCH XU·∫§T:")
                        empty_pages = self.pdf_content.count("[TRANG TR·ªêNG HO·∫∂C KH√îNG C√ì TEXT]")
                        if error_pages > 0:
                            print(f"   - ‚ö†Ô∏è Trang l·ªói: {error_pages}")
                        if empty_pages > 0:
                            print(f"   - üìù Trang tr·ªëng: {empty_pages}")
                    continue
                
                if question.lower() == 'history':
                    self.show_conversation_history()
                    continue
                
                if question.lower() == 'clear':
                    self.conversation_history.clear()
                    print("üóëÔ∏è ƒê√£ x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán")
                    continue
                
                if question.lower() == 'cache':
                    self.list_cache_files()
                    continue
                
                if not question:
                    print("‚ö†Ô∏è Vui l√≤ng nh·∫≠p c√¢u h·ªèi!")
                    continue
                
                # T·∫°o c√¢u tr·∫£ l·ªùi
                answer, relevant_chunks = self.generate_answer(question)
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                print("\n" + "="*50)
                print("üìã C√ÇU TR·∫¢ L·ªúI T·ª™ T√ÄI LI·ªÜU:")
                print("="*50)
                print(answer)
                
                # Hi·ªÉn th·ªã th√¥ng tin chunks ƒë∆∞·ª£c s·ª≠ d·ª•ng
                print(f"\nüìö ƒê√£ s·ª≠ d·ª•ng {len(relevant_chunks)} ph·∫ßn th√¥ng tin li√™n quan")
                for i, chunk in enumerate(relevant_chunks, 1):
                    print(f"   {i}. ƒê·ªô li√™n quan: {chunk['similarity']:.2f}")
                
                # T·∫°o ki·∫øn th·ª©c m·ªü r·ªông
                extended = self.generate_extended_knowledge(question, answer)
                
                print("\n" + "="*50)
                print("üåê KI·∫æN TH·ª®C M·ªû R·ªòNG:")
                print("="*50)
                print(extended)
                
                # Th√™m v√†o l·ªãch s·ª≠ tr√≤ chuy·ªán
                self.add_to_history(question, answer, extended)
                
                # Auto-save sau m·ªói 3 c√¢u h·ªèi
                if len(self.conversation_history) % 3 == 0:
                    self.save_conversation_history()
                
            except KeyboardInterrupt:
                self.save_conversation_history()
                print("\nüëã T·∫°m bi·ªát!")
                break
            except Exception as e:
                print(f"‚ùå L·ªói: {str(e)}")

def main():
    """H√†m ch√≠nh v·ªõi cache management c·∫£i ti·∫øn"""
    rag_system = PDFChatRAG()
    
    # Hi·ªÉn th·ªã cache c√≥ s·∫µn
    available_caches = rag_system.list_cache_files()
    
    # T√¨m file PDF
    pdf_files = list(Path(".").glob("*.pdf"))
    
    if pdf_files:
        print("\nüìã File PDF t√¨m th·∫•y:")
        for i, pdf_file in enumerate(pdf_files, 1):
            file_size = os.path.getsize(pdf_file) / 1024
            
            # Ki·ªÉm tra c√≥ cache kh√¥ng
            cache_filename = rag_system.get_cache_filename(str(pdf_file))
            has_cache = os.path.exists(cache_filename)
            status = "üíæ" if has_cache else "üÜï"
            
            print(f"   {i}. {status} {pdf_file.name} ({file_size:.1f} KB)")
        
        try:
            choice = input(f"\nüî¢ Ch·ªçn file (1-{len(pdf_files)}) ho·∫∑c nh·∫≠p ƒë∆∞·ªùng d·∫´n: ").strip()
            
            if choice.isdigit() and 1 <= int(choice) <= len(pdf_files):
                pdf_path = str(pdf_files[int(choice) - 1])
            else:
                pdf_path = choice
                
        except ValueError:
            pdf_path = choice
    else:
        pdf_path = input("üìÅ Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn file PDF: ").strip()
    
    if not pdf_path or not os.path.exists(pdf_path):
        print("‚ùå File PDF kh√¥ng t·ªìn t·∫°i!")
        return
    
    rag_system.current_pdf_path = pdf_path
    
    # Th·ª≠ t·∫£i cache cho PDF n√†y
    if rag_system.load_cache(pdf_path):
        print("üöÄ ƒê√£ s·∫µn s√†ng chat v·ªõi cache!")
        rag_system.chat()
        return
    
    # N·∫øu kh√¥ng c√≥ cache, x·ª≠ l√Ω PDF m·ªõi
    print("üîÑ ƒêang x·ª≠ l√Ω PDF m·ªõi...")
    if rag_system.convert_pdf_to_text(pdf_path):
        if rag_system.create_embeddings():
            print("üéâ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")
            rag_system.chat()
        else:
            print("‚ùå L·ªói t·∫°o embeddings")
    else:
        print("‚ùå L·ªói x·ª≠ l√Ω PDF")

if __name__ == "__main__":
    main()