import os
import json
import requests
from pathlib import Path
from dotenv import load_dotenv
import subprocess
import sys
import hashlib
from datetime import datetime

# Import cho RAG
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle
except ImportError:
    print("âŒ Thiáº¿u má»™t sá»‘ thÆ° viá»‡n cáº§n thiáº¿t. Äang cÃ i Ä‘áº·t...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers", "scikit-learn", "numpy"])
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle

# Load environment variables
load_dotenv()

class PDFChatRAG:
    def __init__(self):
        """Khá»Ÿi táº¡o há»‡ thá»‘ng RAG cho PDF chat"""
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y OPENAI_API_KEY trong file .env")
            sys.exit(1)
        
        # Khá»Ÿi táº¡o model embedding
        print("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh embedding...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Biáº¿n lÆ°u trá»¯
        self.chunks = []
        self.embeddings = []
        self.pdf_content = ""
        
        # THÃŠM: Quáº£n lÃ½ cache vÃ  lá»‹ch sá»­
        self.current_pdf_path = None
        self.conversation_history = []
        self.max_history = 10
        
        print("âœ… Há»‡ thá»‘ng RAG Ä‘Ã£ sáºµn sÃ ng!")
    
    def convert_pdf_to_text(self, pdf_path):
        """Chuyá»ƒn Ä‘á»•i PDF sang text báº±ng pdfToText.py cáº£i tiáº¿n"""
        try:
            print(f"ğŸ“„ Äang chuyá»ƒn Ä‘á»•i PDF: {pdf_path}")
            
            # Import vÃ  sá»­ dá»¥ng function tá»« pdfToText.py
            import importlib.util
            spec = importlib.util.spec_from_file_location("pdfToText", "pdfToText.py")
            pdf_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(pdf_module)
            
            # Chuyá»ƒn Ä‘á»•i PDF vá»›i error handling nÃ¢ng cao
            temp_output = f"temp_pdf_content_{hashlib.md5(pdf_path.encode()).hexdigest()[:8]}.txt"
            
            print("ğŸ”§ Sá»­ dá»¥ng PDF converter cáº£i tiáº¿n...")
            success = pdf_module.pdf_to_text(pdf_path, temp_output)
            
            if success and os.path.exists(temp_output):
                with open(temp_output, 'r', encoding='utf-8') as f:
                    self.pdf_content = f.read()
                
                # XÃ³a file táº¡m
                os.remove(temp_output)
                
                # Kiá»ƒm tra cháº¥t lÆ°á»£ng ná»™i dung
                if len(self.pdf_content.strip()) == 0:
                    print("âš ï¸ Cáº£nh bÃ¡o: PDF khÃ´ng chá»©a text cÃ³ thá»ƒ Ä‘á»c Ä‘Æ°á»£c")
                    return False
                
                # Äáº¿m cÃ¡c trang lá»—i
                error_pages = self.pdf_content.count("[Lá»–I TRÃCH XUáº¤T:")
                empty_pages = self.pdf_content.count("[TRANG TRá»NG HOáº¶C KHÃ”NG CÃ“ TEXT]")
                
                if error_pages > 0:
                    print(f"âš ï¸ CÃ³ {error_pages} trang bá»‹ lá»—i khi trÃ­ch xuáº¥t")
                if empty_pages > 0:
                    print(f"â„¹ï¸ CÃ³ {empty_pages} trang trá»‘ng")
                
                print("âœ… Chuyá»ƒn Ä‘á»•i PDF thÃ nh cÃ´ng!")
                return True
            else:
                print("âŒ Lá»—i chuyá»ƒn Ä‘á»•i PDF")
                return False
                
        except ImportError as e:
            print(f"âŒ Lá»—i import pdfToText.py: {str(e)}")
            print("ğŸ’¡ Äáº£m báº£o file pdfToText.py cÃ³ trong thÆ° má»¥c nÃ y")
            return False
        except Exception as e:
            print(f"âŒ Lá»—i: {str(e)}")
            return False
    
    def create_chunks(self, text, chunk_size=500, overlap=50):
        """Chia text thÃ nh cÃ¡c chunks nhá» vá»›i overlap vÃ  lÃ m sáº¡ch dá»¯ liá»‡u"""
        # LÃ m sáº¡ch text trÆ°á»›c khi chia chunks
        cleaned_text = self.clean_text(text)
        
        words = cleaned_text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            
            # Chá»‰ thÃªm chunk náº¿u cÃ³ ná»™i dung há»¯u Ã­ch
            if len(chunk.strip()) > 50 and not self.is_error_chunk(chunk):
                chunks.append(chunk.strip())
        
        print(f"ğŸ“š ÄÃ£ táº¡o {len(chunks)} chunks há»¯u Ã­ch tá»« tÃ i liá»‡u")
        return chunks
    
    def clean_text(self, text):
        """LÃ m sáº¡ch text vÃ  loáº¡i bá» cÃ¡c pháº§n khÃ´ng cáº§n thiáº¿t"""
        import re
        
        # Loáº¡i bá» header trang
        text = re.sub(r'={50}\nTRANG \d+\n={50}', '', text)
        
        # Loáº¡i bá» cÃ¡c dÃ²ng trá»‘ng liÃªn tiáº¿p
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def is_error_chunk(self, chunk):
        """Kiá»ƒm tra xem chunk cÃ³ pháº£i lÃ  lá»—i khÃ´ng"""
        error_indicators = [
            "[Lá»–I TRÃCH XUáº¤T:",
            "[TRANG TRá»NG HOáº¶C KHÃ”NG CÃ“ TEXT]",
            "[TRANG", ": KHÃ”NG THá»‚ TRÃCH XUáº¤T TEXT]"
        ]
        
        return any(indicator in chunk for indicator in error_indicators)
    
    def create_embeddings(self):
        """Táº¡o embeddings cho cÃ¡c chunks"""
        if not self.pdf_content:
            print("âŒ ChÆ°a cÃ³ ná»™i dung PDF Ä‘á»ƒ xá»­ lÃ½")
            return False
        
        print("ğŸ”„ Äang táº¡o chunks vÃ  embeddings...")
        
        # Táº¡o chunks
        self.chunks = self.create_chunks(self.pdf_content)
        
        # Táº¡o embeddings
        self.embeddings = self.embedding_model.encode(self.chunks)
        
        print(f"âœ… ÄÃ£ táº¡o embeddings cho {len(self.chunks)} chunks")
        
        # LÆ°u cache vá»›i tÃªn file riÃªng
        if self.current_pdf_path:
            self.save_cache()
        
        return True
    
    def save_cache(self):
        """LÆ°u cache vá»›i tÃªn file riÃªng"""
        try:
            cache_filename = self.get_cache_filename(self.current_pdf_path)
            
            cache_data = {
                'pdf_path': self.current_pdf_path,
                'pdf_modified_time': os.path.getmtime(self.current_pdf_path),
                'chunks': self.chunks,
                'embeddings': self.embeddings.tolist(),
                'pdf_content': self.pdf_content
            }
            
            with open(cache_filename, 'wb') as f:
                pickle.dump(cache_data, f)
            
            print(f"ğŸ’¾ ÄÃ£ lÆ°u cache: {cache_filename}")
            return True
            
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u cache: {str(e)}")
            return False
    
    def load_cache(self, pdf_path):
        """Táº£i cache cho PDF cá»¥ thá»ƒ"""
        try:
            cache_filename = self.get_cache_filename(pdf_path)
            
            if not os.path.exists(cache_filename):
                print(f"â„¹ï¸ ChÆ°a cÃ³ cache cho PDF: {Path(pdf_path).name}")
                return False
            
            with open(cache_filename, 'rb') as f:
                cache_data = pickle.load(f)
            
            # Kiá»ƒm tra PDF cÃ³ thay Ä‘á»•i khÃ´ng
            if os.path.exists(pdf_path):
                current_modified_time = os.path.getmtime(pdf_path)
                cached_modified_time = cache_data.get('pdf_modified_time', 0)
                
                if abs(current_modified_time - cached_modified_time) > 1:
                    print("âš ï¸ PDF Ä‘Ã£ thay Ä‘á»•i, cache khÃ´ng cÃ²n há»£p lá»‡")
                    return False
            
            # Táº£i dá»¯ liá»‡u tá»« cache
            self.chunks = cache_data['chunks']
            self.embeddings = np.array(cache_data['embeddings'])
            self.pdf_content = cache_data['pdf_content']
            self.current_pdf_path = pdf_path
            
            print(f"âœ… ÄÃ£ táº£i cache: {cache_filename}")
            return True
            
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ táº£i cache: {str(e)}")
            return False
    
    def retrieve_relevant_chunks(self, query, top_k=3):
        """TÃ¬m kiáº¿m chunks liÃªn quan nháº¥t vá»›i cÃ¢u há»i"""
        if not self.chunks:
            return []
        
        # Táº¡o embedding cho query
        query_embedding = self.embedding_model.encode([query])
        
        # TÃ­nh cosine similarity
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # Láº¥y top_k chunks cÃ³ similarity cao nháº¥t
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        relevant_chunks = []
        for idx in top_indices:
            relevant_chunks.append({
                'text': self.chunks[idx],
                'similarity': similarities[idx]
            })
        
        return relevant_chunks
    
    def call_openai_api(self, prompt):
        """Gá»i OpenAI API"""
        try:
            url = "https://api.openai.com/v1/chat/completions"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.openai_api_key}"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.7
            }
            
            response = requests.post(url, headers=headers, json=data)
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                return "âŒ Lá»—i: KhÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i tá»« OpenAI"
                
        except Exception as e:
            return f"âŒ Lá»—i gá»i API: {str(e)}"
    
    def generate_answer(self, question):
        """Táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn RAG vá»›i context tá»« lá»‹ch sá»­"""
        print(f"ğŸ” Äang tÃ¬m kiáº¿m thÃ´ng tin liÃªn quan...")
        
        # Retrieve relevant chunks
        relevant_chunks = self.retrieve_relevant_chunks(question, top_k=3)
        
        if not relevant_chunks:
            return "âŒ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u", []
        
        # Táº¡o context tá»« relevant chunks
        document_context = "\n\n".join([chunk['text'] for chunk in relevant_chunks])
        
        # Láº¥y context tá»« lá»‹ch sá»­ trÃ² chuyá»‡n
        conversation_context = self.get_conversation_context(question)
        
        # Táº¡o prompt vá»›i context Ä‘áº§y Ä‘á»§
        prompt = f"""
{conversation_context}

THÃ”NG TIN TÃ€I LIá»†U LIÃŠN QUAN:
{document_context}

HÃ£y tráº£ lá»i cÃ¢u há»i hiá»‡n táº¡i dá»±a trÃªn:
1. ThÃ´ng tin trong tÃ i liá»‡u
2. Bá»‘i cáº£nh tá»« cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³ (náº¿u cÃ³)

Náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n cuá»™c trÃ² chuyá»‡n trÆ°á»›c, hÃ£y tham chiáº¿u Ä‘áº¿n nÃ³ má»™t cÃ¡ch tá»± nhiÃªn.

TRáº¢ Lá»œI:
"""
        
        print("ğŸ¤– Äang táº¡o cÃ¢u tráº£ lá»i...")
        answer = self.call_openai_api(prompt)
        
        return answer, relevant_chunks
    
    def generate_extended_knowledge(self, question, answer):
        """Táº¡o kiáº¿n thá»©c má»Ÿ rá»™ng tá»« nguá»“n bÃªn ngoÃ i"""
        prompt = f"""
CÃ¢u há»i gá»‘c: {question}
CÃ¢u tráº£ lá»i tá»« tÃ i liá»‡u: {answer}

HÃ£y cung cáº¥p thÃªm kiáº¿n thá»©c má»Ÿ rá»™ng, thÃ´ng tin bá»• sung, hoáº·c cÃ¡c khÃ­a cáº¡nh liÃªn quan khÃ¡c vá» chá»§ Ä‘á» nÃ y tá»« kiáº¿n thá»©c tá»•ng quÃ¡t cá»§a báº¡n. Äá»«ng láº·p láº¡i thÃ´ng tin Ä‘Ã£ cÃ³ trong cÃ¢u tráº£ lá»i gá»‘c.

KIáº¾N THá»¨C Má» Rá»˜NG:
"""
        
        print("ğŸŒ Äang táº¡o kiáº¿n thá»©c má»Ÿ rá»™ng...")
        extended_knowledge = self.call_openai_api(prompt)
        
        return extended_knowledge
    
    def get_cache_filename(self, pdf_path):
        """Táº¡o tÃªn cache riÃªng cho tá»«ng PDF"""
        # Láº¥y tÃªn file PDF
        pdf_name = Path(pdf_path).stem
        
        # Táº¡o hash tá»« Ä‘Æ°á»ng dáº«n Ä‘á»ƒ trÃ¡nh trÃ¹ng tÃªn
        path_hash = hashlib.md5(pdf_path.encode()).hexdigest()[:8]
        
        return f"cache_{pdf_name}_{path_hash}.pkl"
    
    def add_to_history(self, question, answer, extended_knowledge=None):
        """ThÃªm cÃ¢u há»i vÃ  tráº£ lá»i vÃ o lá»‹ch sá»­"""
        conversation_item = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'question': question,
            'answer': answer,
            'extended_knowledge': extended_knowledge
        }
        
        self.conversation_history.append(conversation_item)
        
        # Giá»›i háº¡n Ä‘á»™ dÃ i lá»‹ch sá»­
        if len(self.conversation_history) > self.max_history:
            self.conversation_history.pop(0)
    
    def get_conversation_context(self, current_question):
        """Láº¥y context tá»« lá»‹ch sá»­ trÃ² chuyá»‡n"""
        if not self.conversation_history:
            return ""
        
        context = "\n=== Lá»ŠCH Sá»¬ TRÃ’ CHUYá»†N Gáº¦N ÄÃ‚Y ===\n"
        
        # Láº¥y 3 cuá»™c trÃ² chuyá»‡n gáº§n nháº¥t
        recent_conversations = self.conversation_history[-3:]
        
        for i, conv in enumerate(recent_conversations, 1):
            context += f"\nCÃ¢u há»i {i}: {conv['question']}\n"
            context += f"Tráº£ lá»i {i}: {conv['answer'][:200]}...\n"  # Cáº¯t ngáº¯n Ä‘á»ƒ tiáº¿t kiá»‡m token
        
        context += f"\n=== CÃ‚U Há»I HIá»†N Táº I ===\n{current_question}\n"
        
        return context
    
    def save_conversation_history(self, filename="conversation_history.json"):
        """LÆ°u lá»‹ch sá»­ trÃ² chuyá»‡n ra file"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u lá»‹ch sá»­ trÃ² chuyá»‡n vÃ o {filename}")
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u lá»‹ch sá»­: {str(e)}")
    
    def load_conversation_history(self, filename="conversation_history.json"):
        """Táº£i lá»‹ch sá»­ trÃ² chuyá»‡n tá»« file"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    self.conversation_history = json.load(f)
                print(f"ğŸ“š ÄÃ£ táº£i {len(self.conversation_history)} cuá»™c trÃ² chuyá»‡n tá»« lá»‹ch sá»­")
                return True
        except Exception as e:
            print(f"âš ï¸ Lá»—i táº£i lá»‹ch sá»­: {str(e)}")
        return False
    
    def show_conversation_history(self):
        """Hiá»ƒn thá»‹ lá»‹ch sá»­ trÃ² chuyá»‡n"""
        if not self.conversation_history:
            print("ğŸ“ ChÆ°a cÃ³ lá»‹ch sá»­ trÃ² chuyá»‡n")
            return
        
        print(f"\nğŸ“š Lá»ŠCH Sá»¬ TRÃ’ CHUYá»†N ({len(self.conversation_history)} cuá»™c trÃ² chuyá»‡n):")
        print("="*60)
        
        for i, conv in enumerate(self.conversation_history, 1):
            print(f"\nğŸ• {conv['timestamp']}")
            print(f"â“ CÃ¢u há»i {i}: {conv['question']}")
            print(f"ğŸ’¬ Tráº£ lá»i: {conv['answer'][:150]}...")
            if conv.get('extended_knowledge'):
                print(f"ğŸŒ Má»Ÿ rá»™ng: {conv['extended_knowledge'][:100]}...")
            print("-" * 40)
    
    def list_cache_files(self):
        """Liá»‡t kÃª cÃ¡c file cache cÃ³ sáºµn"""
        cache_files = list(Path(".").glob("cache_*.pkl"))
        
        if not cache_files:
            print("ğŸ“ ChÆ°a cÃ³ cache nÃ o")
            return []
        
        print("ğŸ“š Cache cÃ³ sáºµn:")
        cache_info = []
        
        for cache_file in cache_files:
            try:
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                
                pdf_name = Path(cache_data.get('pdf_path', 'Unknown')).name
                modified_time = cache_data.get('pdf_modified_time', 0)
                
                cache_info.append({
                    'file': cache_file,
                    'pdf_path': cache_data.get('pdf_path', ''),
                    'pdf_name': pdf_name
                })
                
                print(f"   ğŸ“„ {pdf_name}")
                print(f"      Cache: {cache_file.name}")
                print(f"      Size: {cache_file.stat().st_size / 1024:.1f} KB")
                if modified_time:
                    print(f"      Thá»i gian: {datetime.fromtimestamp(modified_time)}")
                
            except Exception:
                print(f"   âŒ Cache lá»—i: {cache_file.name}")
        
        return cache_info
    
    def chat(self):
        """Báº¯t Ä‘áº§u chat loop vá»›i lá»‹ch sá»­ trÃ² chuyá»‡n"""
        # Táº£i lá»‹ch sá»­ trÃ² chuyá»‡n náº¿u cÃ³
        self.load_conversation_history()
        
        print("\n" + "="*60)
        print("ğŸ¤– PDF CHAT BOT vá»›i RAG & Lá»ŠCH Sá»¬ TRÃ’ CHUYá»†N")
        print("="*60)
        print("ğŸ’¡ Nháº­p 'quit' Ä‘á»ƒ thoÃ¡t")
        print("ğŸ’¡ Nháº­p 'info' Ä‘á»ƒ xem thÃ´ng tin tÃ i liá»‡u")
        print("ğŸ’¡ Nháº­p 'history' Ä‘á»ƒ xem lá»‹ch sá»­ trÃ² chuyá»‡n")
        print("ğŸ’¡ Nháº­p 'clear' Ä‘á»ƒ xÃ³a lá»‹ch sá»­ trÃ² chuyá»‡n")
        print("ğŸ’¡ Nháº­p 'cache' Ä‘á»ƒ xem cÃ¡c file cache cÃ³ sáºµn")
        print("-"*60)
        
        while True:
            try:
                question = input("\nâ“ CÃ¢u há»i cá»§a báº¡n: ").strip()
                
                if question.lower() in ['quit', 'exit', 'q']:
                    # LÆ°u lá»‹ch sá»­ trÆ°á»›c khi thoÃ¡t
                    self.save_conversation_history()
                    print("ğŸ‘‹ Táº¡m biá»‡t!")
                    break
                
                if question.lower() == 'info':
                    print(f"ğŸ“Š ThÃ´ng tin tÃ i liá»‡u:")
                    print(f"   - Sá»‘ chunks: {len(self.chunks)}")
                    print(f"   - Äá»™ dÃ i ná»™i dung: {len(self.pdf_content):,} kÃ½ tá»±")
                    print(f"   - Sá»‘ cuá»™c trÃ² chuyá»‡n: {len(self.conversation_history)}")
                    if self.current_pdf_path:
                        print(f"   - File PDF: {Path(self.current_pdf_path).name}")
                        
                        # ThÃªm thá»‘ng kÃª cháº¥t lÆ°á»£ng
                        error_pages = self.pdf_content.count("[Lá»–I TRÃCH XUáº¤T:")
                        empty_pages = self.pdf_content.count("[TRANG TRá»NG HOáº¶C KHÃ”NG CÃ“ TEXT]")
                        if error_pages > 0:
                            print(f"   - âš ï¸ Trang lá»—i: {error_pages}")
                        if empty_pages > 0:
                            print(f"   - ğŸ“ Trang trá»‘ng: {empty_pages}")
                    continue
                
                if question.lower() == 'history':
                    self.show_conversation_history()
                    continue
                
                if question.lower() == 'clear':
                    self.conversation_history.clear()
                    print("ğŸ—‘ï¸ ÄÃ£ xÃ³a lá»‹ch sá»­ trÃ² chuyá»‡n")
                    continue
                
                if question.lower() == 'cache':
                    self.list_cache_files()
                    continue
                
                if not question:
                    print("âš ï¸ Vui lÃ²ng nháº­p cÃ¢u há»i!")
                    continue
                
                # Táº¡o cÃ¢u tráº£ lá»i
                answer, relevant_chunks = self.generate_answer(question)
                
                # Hiá»ƒn thá»‹ káº¿t quáº£
                print("\n" + "="*50)
                print("ğŸ“‹ CÃ‚U TRáº¢ Lá»œI Tá»ª TÃ€I LIá»†U:")
                print("="*50)
                print(answer)
                
                # Hiá»ƒn thá»‹ thÃ´ng tin chunks Ä‘Æ°á»£c sá»­ dá»¥ng
                print(f"\nğŸ“š ÄÃ£ sá»­ dá»¥ng {len(relevant_chunks)} pháº§n thÃ´ng tin liÃªn quan")
                for i, chunk in enumerate(relevant_chunks, 1):
                    print(f"   {i}. Äá»™ liÃªn quan: {chunk['similarity']:.2f}")
                
                # Táº¡o kiáº¿n thá»©c má»Ÿ rá»™ng
                extended = self.generate_extended_knowledge(question, answer)
                
                print("\n" + "="*50)
                print("ğŸŒ KIáº¾N THá»¨C Má» Rá»˜NG:")
                print("="*50)
                print(extended)
                
                # ThÃªm vÃ o lá»‹ch sá»­ trÃ² chuyá»‡n
                self.add_to_history(question, answer, extended)
                
                # Auto-save sau má»—i 3 cÃ¢u há»i
                if len(self.conversation_history) % 3 == 0:
                    self.save_conversation_history()
                
            except KeyboardInterrupt:
                self.save_conversation_history()
                print("\nğŸ‘‹ Táº¡m biá»‡t!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {str(e)}")

def main():
    """HÃ m chÃ­nh vá»›i cache management cáº£i tiáº¿n"""
    rag_system = PDFChatRAG()
    
    # Hiá»ƒn thá»‹ cache cÃ³ sáºµn
    available_caches = rag_system.list_cache_files()
    
    # TÃ¬m file PDF
    pdf_files = list(Path(".").glob("*.pdf"))
    
    if pdf_files:
        print("\nğŸ“‹ File PDF tÃ¬m tháº¥y:")
        for i, pdf_file in enumerate(pdf_files, 1):
            file_size = os.path.getsize(pdf_file) / 1024
            
            # Kiá»ƒm tra cÃ³ cache khÃ´ng
            cache_filename = rag_system.get_cache_filename(str(pdf_file))
            has_cache = os.path.exists(cache_filename)
            status = "ğŸ’¾" if has_cache else "ğŸ†•"
            
            print(f"   {i}. {status} {pdf_file.name} ({file_size:.1f} KB)")
        
        try:
            choice = input(f"\nğŸ”¢ Chá»n file (1-{len(pdf_files)}) hoáº·c nháº­p Ä‘Æ°á»ng dáº«n: ").strip()
            
            if choice.isdigit() and 1 <= int(choice) <= len(pdf_files):
                pdf_path = str(pdf_files[int(choice) - 1])
            else:
                pdf_path = choice
                
        except ValueError:
            pdf_path = choice
    else:
        pdf_path = input("ğŸ“ Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file PDF: ").strip()
    
    if not pdf_path or not os.path.exists(pdf_path):
        print("âŒ File PDF khÃ´ng tá»“n táº¡i!")
        return
    
    rag_system.current_pdf_path = pdf_path
    
    # Thá»­ táº£i cache cho PDF nÃ y
    if rag_system.load_cache(pdf_path):
        print("ğŸš€ ÄÃ£ sáºµn sÃ ng chat vá»›i cache!")
        rag_system.chat()
        return
    
    # Náº¿u khÃ´ng cÃ³ cache, xá»­ lÃ½ PDF má»›i
    print("ğŸ”„ Äang xá»­ lÃ½ PDF má»›i...")
    if rag_system.convert_pdf_to_text(pdf_path):
        if rag_system.create_embeddings():
            print("ğŸ‰ Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng!")
            rag_system.chat()
        else:
            print("âŒ Lá»—i táº¡o embeddings")
    else:
        print("âŒ Lá»—i xá»­ lÃ½ PDF")

if __name__ == "__main__":
    main()