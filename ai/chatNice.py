import os
import json
import requests
from pathlib import Path
from dotenv import load_dotenv
import subprocess
import sys

# Import cho RAG
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle
except ImportError:
    print("âŒ Thiáº¿u má»™t sá»‘ thÆ° viá»‡n cáº§n thiáº¿t. Äang cÃ i Ä‘áº·t...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers", "scikit-learn", "numpy"])
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import pickle

# Load environment variables
load_dotenv()

class PDFChatRAG:
    def __init__(self):
        """Khá»Ÿi táº¡o há»‡ thá»‘ng RAG cho PDF chat"""
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y OPENAI_API_KEY trong file .env")
            sys.exit(1)
        
        # Khá»Ÿi táº¡o model embedding
        print("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh embedding...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Biáº¿n lÆ°u trá»¯
        self.chunks = []
        self.embeddings = []
        self.pdf_content = ""
        
        print("âœ… Há»‡ thá»‘ng RAG Ä‘Ã£ sáºµn sÃ ng!")
    
    def convert_pdf_to_text(self, pdf_path):
        """Chuyá»ƒn Ä‘á»•i PDF sang text báº±ng pdfToText.py"""
        try:
            print(f"ğŸ“„ Äang chuyá»ƒn Ä‘á»•i PDF: {pdf_path}")
            
            # Import vÃ  sá»­ dá»¥ng function tá»« pdfToText.py
            import importlib.util
            spec = importlib.util.spec_from_file_location("pdfToText", "pdfToText.py")
            pdf_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(pdf_module)
            
            # Chuyá»ƒn Ä‘á»•i PDF
            success = pdf_module.pdf_to_text(pdf_path, "temp_pdf_content.txt")
            
            if success and os.path.exists("temp_pdf_content.txt"):
                with open("temp_pdf_content.txt", 'r', encoding='utf-8') as f:
                    self.pdf_content = f.read()
                
                # XÃ³a file táº¡m
                os.remove("temp_pdf_content.txt")
                print("âœ… Chuyá»ƒn Ä‘á»•i PDF thÃ nh cÃ´ng!")
                return True
            else:
                print("âŒ Lá»—i chuyá»ƒn Ä‘á»•i PDF")
                return False
                
        except Exception as e:
            print(f"âŒ Lá»—i: {str(e)}")
            return False
    
    def create_chunks(self, text, chunk_size=500, overlap=50):
        """Chia text thÃ nh cÃ¡c chunks nhá» vá»›i overlap"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if len(chunk.strip()) > 0:
                chunks.append(chunk.strip())
        
        print(f"ğŸ“š ÄÃ£ táº¡o {len(chunks)} chunks tá»« tÃ i liá»‡u")
        return chunks
    
    def create_embeddings(self):
        """Táº¡o embeddings cho cÃ¡c chunks"""
        if not self.pdf_content:
            print("âŒ ChÆ°a cÃ³ ná»™i dung PDF Ä‘á»ƒ xá»­ lÃ½")
            return False
        
        print("ğŸ”„ Äang táº¡o chunks vÃ  embeddings...")
        
        # Táº¡o chunks
        self.chunks = self.create_chunks(self.pdf_content)
        
        # Táº¡o embeddings
        self.embeddings = self.embedding_model.encode(self.chunks)
        
        print(f"âœ… ÄÃ£ táº¡o embeddings cho {len(self.chunks)} chunks")
        
        # LÆ°u cache
        cache_data = {
            'chunks': self.chunks,
            'embeddings': self.embeddings.tolist(),
            'pdf_content': self.pdf_content
        }
        
        with open('rag_cache.pkl', 'wb') as f:
            pickle.dump(cache_data, f)
        
        return True
    
    def load_cache(self):
        """Táº£i cache náº¿u cÃ³"""
        try:
            if os.path.exists('rag_cache.pkl'):
                with open('rag_cache.pkl', 'rb') as f:
                    cache_data = pickle.load(f)
                
                self.chunks = cache_data['chunks']
                self.embeddings = np.array(cache_data['embeddings'])
                self.pdf_content = cache_data['pdf_content']
                
                print("âœ… ÄÃ£ táº£i cache thÃ nh cÃ´ng!")
                return True
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ táº£i cache: {str(e)}")
        
        return False
    
    def retrieve_relevant_chunks(self, query, top_k=3):
        """TÃ¬m kiáº¿m chunks liÃªn quan nháº¥t vá»›i cÃ¢u há»i"""
        if not self.chunks:
            return []
        
        # Táº¡o embedding cho query
        query_embedding = self.embedding_model.encode([query])
        
        # TÃ­nh cosine similarity
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # Láº¥y top_k chunks cÃ³ similarity cao nháº¥t
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        relevant_chunks = []
        for idx in top_indices:
            relevant_chunks.append({
                'text': self.chunks[idx],
                'similarity': similarities[idx]
            })
        
        return relevant_chunks
    
    def call_openai_api(self, prompt):
        """Gá»i OpenAI API"""
        try:
            url = "https://api.openai.com/v1/chat/completions"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.openai_api_key}"
            }
            
            data = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.7
            }
            
            response = requests.post(url, headers=headers, json=data)
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                return "âŒ Lá»—i: KhÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i tá»« OpenAI"
                
        except Exception as e:
            return f"âŒ Lá»—i gá»i API: {str(e)}"
    
    def generate_answer(self, question):
        """Táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn RAG"""
        print(f"ğŸ” Äang tÃ¬m kiáº¿m thÃ´ng tin liÃªn quan...")
        
        # Retrieve relevant chunks
        relevant_chunks = self.retrieve_relevant_chunks(question, top_k=3)
        
        if not relevant_chunks:
            return "âŒ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u"
        
        # Táº¡o context tá»« relevant chunks
        context = "\n\n".join([chunk['text'] for chunk in relevant_chunks])
        
        # Táº¡o prompt
        prompt = f"""
Dá»±a trÃªn thÃ´ng tin sau Ä‘Ã¢y tá»« tÃ i liá»‡u:

THÃ”NG TIN TÃ€I LIá»†U:
{context}

CÃ‚U Há»I: {question}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin trong tÃ i liá»‡u. Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§ Ä‘á»ƒ tráº£ lá»i, hÃ£y nÃ³i rÃµ Ä‘iá»u Ä‘Ã³.

TRáº¢ Lá»œI:
"""
        
        print("ğŸ¤– Äang táº¡o cÃ¢u tráº£ lá»i...")
        answer = self.call_openai_api(prompt)
        
        return answer, relevant_chunks
    
    def generate_extended_knowledge(self, question, answer):
        """Táº¡o kiáº¿n thá»©c má»Ÿ rá»™ng tá»« nguá»“n bÃªn ngoÃ i"""
        prompt = f"""
CÃ¢u há»i gá»‘c: {question}
CÃ¢u tráº£ lá»i tá»« tÃ i liá»‡u: {answer}

HÃ£y cung cáº¥p thÃªm kiáº¿n thá»©c má»Ÿ rá»™ng, thÃ´ng tin bá»• sung, hoáº·c cÃ¡c khÃ­a cáº¡nh liÃªn quan khÃ¡c vá» chá»§ Ä‘á» nÃ y tá»« kiáº¿n thá»©c tá»•ng quÃ¡t cá»§a báº¡n. Äá»«ng láº·p láº¡i thÃ´ng tin Ä‘Ã£ cÃ³ trong cÃ¢u tráº£ lá»i gá»‘c.

KIáº¾N THá»¨C Má» Rá»˜NG:
"""
        
        print("ğŸŒ Äang táº¡o kiáº¿n thá»©c má»Ÿ rá»™ng...")
        extended_knowledge = self.call_openai_api(prompt)
        
        return extended_knowledge
    
    def chat(self):
        """Báº¯t Ä‘áº§u chat loop"""
        print("\n" + "="*60)
        print("ğŸ¤– PDF CHAT BOT vá»›i RAG")
        print("="*60)
        print("ğŸ’¡ Nháº­p 'quit' Ä‘á»ƒ thoÃ¡t")
        print("ğŸ’¡ Nháº­p 'info' Ä‘á»ƒ xem thÃ´ng tin tÃ i liá»‡u")
        print("-"*60)
        
        while True:
            try:
                question = input("\nâ“ CÃ¢u há»i cá»§a báº¡n: ").strip()
                
                if question.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ Táº¡m biá»‡t!")
                    break
                
                if question.lower() == 'info':
                    print(f"ğŸ“Š ThÃ´ng tin tÃ i liá»‡u:")
                    print(f"   - Sá»‘ chunks: {len(self.chunks)}")
                    print(f"   - Äá»™ dÃ i ná»™i dung: {len(self.pdf_content):,} kÃ½ tá»±")
                    continue
                
                if not question:
                    print("âš ï¸ Vui lÃ²ng nháº­p cÃ¢u há»i!")
                    continue
                
                # Táº¡o cÃ¢u tráº£ lá»i
                answer, relevant_chunks = self.generate_answer(question)
                
                # Hiá»ƒn thá»‹ káº¿t quáº£
                print("\n" + "="*50)
                print("ğŸ“‹ CÃ‚U TRáº¢ Lá»œI Tá»ª TÃ€I LIá»†U:")
                print("="*50)
                print(answer)
                
                # Hiá»ƒn thá»‹ thÃ´ng tin chunks Ä‘Æ°á»£c sá»­ dá»¥ng
                print(f"\nğŸ“š ÄÃ£ sá»­ dá»¥ng {len(relevant_chunks)} pháº§n thÃ´ng tin liÃªn quan")
                for i, chunk in enumerate(relevant_chunks, 1):
                    print(f"   {i}. Äá»™ liÃªn quan: {chunk['similarity']:.2f}")
                
                # Táº¡o kiáº¿n thá»©c má»Ÿ rá»™ng
                extended = self.generate_extended_knowledge(question, answer)
                
                print("\n" + "="*50)
                print("ğŸŒ KIáº¾N THá»¨C Má» Rá»˜NG:")
                print("="*50)
                print(extended)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Táº¡m biá»‡t!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {str(e)}")

def main():
    """HÃ m chÃ­nh"""
    rag_system = PDFChatRAG()
    
    # Kiá»ƒm tra xem cÃ³ cache khÃ´ng
    if rag_system.load_cache():
        print("ğŸ“š ÄÃ£ táº£i dá»¯ liá»‡u tá»« cache. CÃ³ thá»ƒ báº¯t Ä‘áº§u chat ngay!")
        choice = input("ğŸ”„ CÃ³ muá»‘n táº£i PDF má»›i khÃ´ng? (y/N): ").strip().lower()
        if choice != 'y':
            rag_system.chat()
            return
    
    # TÃ¬m file PDF
    pdf_files = list(Path(".").glob("*.pdf"))
    
    if pdf_files:
        print("ğŸ“‹ File PDF tÃ¬m tháº¥y:")
        for i, pdf_file in enumerate(pdf_files, 1):
            file_size = os.path.getsize(pdf_file) / 1024
            print(f"   {i}. {pdf_file.name} ({file_size:.1f} KB)")
        
        try:
            choice = input(f"\nğŸ”¢ Chá»n file (1-{len(pdf_files)}) hoáº·c nháº­p Ä‘Æ°á»ng dáº«n: ").strip()
            
            if choice.isdigit() and 1 <= int(choice) <= len(pdf_files):
                pdf_path = str(pdf_files[int(choice) - 1])
            else:
                pdf_path = choice
                
        except ValueError:
            pdf_path = choice
    else:
        pdf_path = input("ğŸ“ Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n file PDF: ").strip()
    
    if not pdf_path:
        print("âŒ KhÃ´ng cÃ³ file PDF Ä‘Æ°á»£c chá»n!")
        return
    
    # Xá»­ lÃ½ PDF
    if rag_system.convert_pdf_to_text(pdf_path):
        if rag_system.create_embeddings():
            print("ğŸ‰ Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng!")
            rag_system.chat()
        else:
            print("âŒ Lá»—i táº¡o embeddings")
    else:
        print("âŒ Lá»—i xá»­ lÃ½ PDF")

if __name__ == "__main__":
    main()